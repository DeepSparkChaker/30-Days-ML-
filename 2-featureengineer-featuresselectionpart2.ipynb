{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CRISP-DM Methodology\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n        1.Determine business objectives\n\n        2.Assess situation\n\n        3.Determine data mining goals\n\n        4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n\n#  Buissness Understanding\n## Step 1: Import helpful libraries","metadata":{}},{"cell_type":"code","source":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:09:54.756777Z","iopub.execute_input":"2021-08-25T15:09:54.757306Z","iopub.status.idle":"2021-08-25T15:10:02.684683Z","shell.execute_reply.started":"2021-08-25T15:09:54.757274Z","shell.execute_reply":"2021-08-25T15:10:02.683725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","metadata":{}},{"cell_type":"code","source":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\ntest = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# Preview the data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:12.977461Z","iopub.execute_input":"2021-08-25T15:17:12.977831Z","iopub.status.idle":"2021-08-25T15:17:15.04096Z","shell.execute_reply.started":"2021-08-25T15:17:12.977802Z","shell.execute_reply":"2021-08-25T15:17:15.03998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA \n\n### Explore the data\n\n    Null Data\n    Categorical data\n    Itrain.isnull().sum().valuess there Text data\n    wich columns will we use\n    IS there outliers that can destory our algo\n    IS there diffrent range of data\n    Curse of dimm...\n\n####  Null Data ","metadata":{}},{"cell_type":"code","source":"train.isnull().sum().values","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:15.412408Z","iopub.execute_input":"2021-08-25T15:17:15.412865Z","iopub.status.idle":"2021-08-25T15:17:15.715389Z","shell.execute_reply.started":"2021-08-25T15:17:15.412829Z","shell.execute_reply":"2021-08-25T15:17:15.714374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Duplicates ","metadata":{}},{"cell_type":"code","source":"train.duplicated(subset='id', keep='first').sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:18.406761Z","iopub.execute_input":"2021-08-25T15:17:18.407109Z","iopub.status.idle":"2021-08-25T15:17:18.434855Z","shell.execute_reply.started":"2021-08-25T15:17:18.40708Z","shell.execute_reply":"2021-08-25T15:17:18.434057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)-len(train.drop_duplicates())","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:22.71252Z","iopub.execute_input":"2021-08-25T15:17:22.712924Z","iopub.status.idle":"2021-08-25T15:17:23.386411Z","shell.execute_reply.started":"2021-08-25T15:17:22.712894Z","shell.execute_reply":"2021-08-25T15:17:23.385359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking unique values\nl=dict(train.nunique())\nprint(l)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:53.814961Z","iopub.execute_input":"2021-08-25T15:17:53.815311Z","iopub.status.idle":"2021-08-25T15:17:54.536668Z","shell.execute_reply.started":"2021-08-25T15:17:53.815279Z","shell.execute_reply":"2021-08-25T15:17:54.535662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.skew()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:18:59.296343Z","iopub.execute_input":"2021-08-25T15:18:59.296722Z","iopub.status.idle":"2021-08-25T15:18:59.668505Z","shell.execute_reply.started":"2021-08-25T15:18:59.296688Z","shell.execute_reply":"2021-08-25T15:18:59.667521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the structure of the data\nprint(train.info())","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:19:02.816777Z","iopub.execute_input":"2021-08-25T15:19:02.817114Z","iopub.status.idle":"2021-08-25T15:19:03.133721Z","shell.execute_reply.started":"2021-08-25T15:19:02.817087Z","shell.execute_reply":"2021-08-25T15:19:03.132631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Stat","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:19:35.647818Z","iopub.execute_input":"2021-08-25T15:19:35.648207Z","iopub.status.idle":"2021-08-25T15:19:35.87705Z","shell.execute_reply.started":"2021-08-25T15:19:35.648159Z","shell.execute_reply":"2021-08-25T15:19:35.876245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert Dtypes : ","metadata":{}},{"cell_type":"code","source":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:20:38.612751Z","iopub.execute_input":"2021-08-25T15:20:38.61336Z","iopub.status.idle":"2021-08-25T15:20:39.092825Z","shell.execute_reply.started":"2021-08-25T15:20:38.613322Z","shell.execute_reply":"2021-08-25T15:20:39.091826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visual Exploratory ","metadata":{}},{"cell_type":"code","source":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:21:46.1031Z","iopub.execute_input":"2021-08-25T15:21:46.103475Z","iopub.status.idle":"2021-08-25T15:21:46.205215Z","shell.execute_reply.started":"2021-08-25T15:21:46.103445Z","shell.execute_reply":"2021-08-25T15:21:46.204296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Num/Cat Features ","metadata":{}},{"cell_type":"code","source":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:22:43.282767Z","iopub.execute_input":"2021-08-25T15:22:43.283094Z","iopub.status.idle":"2021-08-25T15:22:43.340947Z","shell.execute_reply.started":"2021-08-25T15:22:43.283068Z","shell.execute_reply":"2021-08-25T15:22:43.339882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical features distribution\n#### Histograms of numerical features","metadata":{}},{"cell_type":"code","source":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(7, 2,figsize=(20, 24))\nfor feature in num_columns:\n    plt.subplot(7, 2,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:22:46.824577Z","iopub.execute_input":"2021-08-25T15:22:46.824925Z","iopub.status.idle":"2021-08-25T15:23:36.66197Z","shell.execute_reply.started":"2021-08-25T15:22:46.824895Z","shell.execute_reply":"2021-08-25T15:23:36.661247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Histograms of numerical data show a desperation of values with what look like multinomial distributions, also column cont1 seems to have some areas where the distribution becomes kinda discrete and again test numerical data seems to be similar to train numerical data.**\n### Zooming on the correlation between numerical variables and target.","metadata":{}},{"cell_type":"code","source":"train.corr()['target'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:23:55.005741Z","iopub.execute_input":"2021-08-25T15:23:55.006253Z","iopub.status.idle":"2021-08-25T15:23:55.49766Z","shell.execute_reply.started":"2021-08-25T15:23:55.006216Z","shell.execute_reply":"2021-08-25T15:23:55.4965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's clear tat there isn't any clear relation between numerical variables and target.\n\nNow Exploring correlation between all numerical variables. First we get a correlation grid of all numercial variables and target\n","metadata":{}},{"cell_type":"markdown","source":"### Correlation ","metadata":{}},{"cell_type":"code","source":"train.corr().style.background_gradient(cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:25:05.972467Z","iopub.execute_input":"2021-08-25T15:25:05.972881Z","iopub.status.idle":"2021-08-25T15:25:06.301139Z","shell.execute_reply.started":"2021-08-25T15:25:05.972846Z","shell.execute_reply":"2021-08-25T15:25:06.300467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Box plot of numerical columns","metadata":{}},{"cell_type":"code","source":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:25:57.134505Z","iopub.execute_input":"2021-08-25T15:25:57.135017Z","iopub.status.idle":"2021-08-25T15:25:57.849244Z","shell.execute_reply.started":"2021-08-25T15:25:57.134977Z","shell.execute_reply":"2021-08-25T15:25:57.848505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test data ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,6))\nsns.boxplot(data=test[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of test numerical columns', fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:26:25.15205Z","iopub.execute_input":"2021-08-25T15:26:25.152591Z","iopub.status.idle":"2021-08-25T15:26:25.758068Z","shell.execute_reply.started":"2021-08-25T15:26:25.152559Z","shell.execute_reply":"2021-08-25T15:26:25.757411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Numerical Data seems to be kinda normalized with few outliers appearing in the box plot Also test numerical data seems to looks like the train ones.","metadata":{}},{"cell_type":"markdown","source":"## Categorical features distribution","metadata":{}},{"cell_type":"markdown","source":"### Number of categorical unique values","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\nsns.barplot(y=train[cat_columns].nunique().values, x=train[cat_columns].nunique().index, color='blue', alpha=.5)\nplt.xticks(rotation=0)\nplt.title('Number of categorical unique values',fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:27:27.199809Z","iopub.execute_input":"2021-08-25T15:27:27.200312Z","iopub.status.idle":"2021-08-25T15:27:27.50473Z","shell.execute_reply.started":"2021-08-25T15:27:27.20028Z","shell.execute_reply":"2021-08-25T15:27:27.503941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of columns seems to have few categorical unique values except cat9 column.","metadata":{}},{"cell_type":"markdown","source":"### Categorical features distribution","metadata":{}},{"cell_type":"code","source":"labels = train['cat7'].astype('category').cat.categories.tolist()\ncounts = train['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:28:14.384938Z","iopub.execute_input":"2021-08-25T15:28:14.385481Z","iopub.status.idle":"2021-08-25T15:28:14.569409Z","shell.execute_reply.started":"2021-08-25T15:28:14.385426Z","shell.execute_reply":"2021-08-25T15:28:14.568824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 2,figsize=(28, 44))\nfor feature in cat_columns:\n    plt.subplot(5, 2,i)\n    sns.histplot(train[feature],color=\"blue\", label='train')\n    sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:27:54.114834Z","iopub.execute_input":"2021-08-25T15:27:54.115226Z","iopub.status.idle":"2021-08-25T15:28:03.090322Z","shell.execute_reply.started":"2021-08-25T15:27:54.115167Z","shell.execute_reply":"2021-08-25T15:28:03.089279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.violinplot(data = train, y = 'target', x = 'cat'+str(n),ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with categorical features', fontsize=16,y=.93);","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:28:35.348077Z","iopub.execute_input":"2021-08-25T15:28:35.348594Z","iopub.status.idle":"2021-08-25T15:28:46.027029Z","shell.execute_reply.started":"2021-08-25T15:28:35.34854Z","shell.execute_reply":"2021-08-25T15:28:46.026112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  KDE plot of target with categorical features ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.kdeplot(data = train, x = 'target', hue = 'cat'+str(n),ax=ax, alpha =.7, fill=False)\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('KDE plot of train target with categorical features', fontsize=16,y=.93);","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:29:41.629405Z","iopub.execute_input":"2021-08-25T15:29:41.629745Z","iopub.status.idle":"2021-08-25T15:29:58.384194Z","shell.execute_reply.started":"2021-08-25T15:29:41.629718Z","shell.execute_reply":"2021-08-25T15:29:58.383258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot kinda agrees with previous one but it looks like the KDE of some categorical values are pretty much flat compared to other value.","metadata":{}},{"cell_type":"markdown","source":"## Convert Dtypes :","metadata":{}},{"cell_type":"code","source":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:43:15.853586Z","iopub.execute_input":"2021-08-25T15:43:15.853977Z","iopub.status.idle":"2021-08-25T15:43:15.960838Z","shell.execute_reply.started":"2021-08-25T15:43:15.853947Z","shell.execute_reply":"2021-08-25T15:43:15.959859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Target \n###  exploring target data main statistics","metadata":{}},{"cell_type":"code","source":"train['target'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:43:38.374283Z","iopub.execute_input":"2021-08-25T15:43:38.374679Z","iopub.status.idle":"2021-08-25T15:43:38.395924Z","shell.execute_reply.started":"2021-08-25T15:43:38.374645Z","shell.execute_reply":"2021-08-25T15:43:38.394927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].describe().iloc[1:].plot.barh(color=v0,alpha=.5,figsize=(12,5))\nplt.title('Target data statistics',fontsize=16)\nplt.yticks(fontsize=14)\nplt.xticks(np.arange(0,10.8,.5));","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:04:21.042095Z","iopub.execute_input":"2021-08-25T16:04:21.042596Z","iopub.status.idle":"2021-08-25T16:04:21.288551Z","shell.execute_reply.started":"2021-08-25T16:04:21.042562Z","shell.execute_reply":"2021-08-25T16:04:21.287636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Target","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nf, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n\nf.suptitle('Target', fontsize=16)\ng = sns.kdeplot(train['target'], shade=True, label=\"%.2f\"%(train['target'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train['target'], plot=axes[1])\nsns.boxplot(x='target', data=train, orient='h', ax=axes[2]);\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:04:33.235133Z","iopub.execute_input":"2021-08-25T16:04:33.235507Z","iopub.status.idle":"2021-08-25T16:04:36.318501Z","shell.execute_reply.started":"2021-08-25T16:04:33.235474Z","shell.execute_reply":"2021-08-25T16:04:36.317839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n, bins, patches = plt.hist(x=train['target'], bins='auto', color='blue',alpha=0.7, rwidth=0.5)\nplt.xlabel(\"target\")\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-25T16:04:49.262813Z","iopub.execute_input":"2021-08-25T16:04:49.263401Z","iopub.status.idle":"2021-08-25T16:04:49.971867Z","shell.execute_reply.started":"2021-08-25T16:04:49.263364Z","shell.execute_reply":"2021-08-25T16:04:49.971216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Box plot of target data with percentile of .1% and 99.9%","metadata":{}},{"cell_type":"code","source":"y=train['target']\nplt.figure(figsize=(12,6))\nsns.boxplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,.5), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,1), label='1%', c='green', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99.9), label='99.9%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:06:18.958093Z","iopub.execute_input":"2021-08-25T16:06:18.958419Z","iopub.status.idle":"2021-08-25T16:06:19.287233Z","shell.execute_reply.started":"2021-08-25T16:06:18.958394Z","shell.execute_reply":"2021-08-25T16:06:19.286266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bin target ","metadata":{}},{"cell_type":"code","source":"bins = [0.140329,7.742071,  8.728634, 10.411992]\n# Bin labels\nlabels1 = [ 'Low', 'Medium', 'High']\ntrainessai=train.copy()\n# Bin the continuous variable ConvertedSalary using these boundaries\ntrainessai['target_binned'] = pd.cut(trainessai['target'], \n                                bins=bins,labels=labels1 )","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:08:32.349957Z","iopub.execute_input":"2021-08-25T16:08:32.350413Z","iopub.status.idle":"2021-08-25T16:08:32.381491Z","shell.execute_reply.started":"2021-08-25T16:08:32.350361Z","shell.execute_reply":"2021-08-25T16:08:32.380493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = trainessai['target_binned'].astype('category').cat.categories.tolist()\ncounts = trainessai['target_binned'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:08:34.458715Z","iopub.execute_input":"2021-08-25T16:08:34.459054Z","iopub.status.idle":"2021-08-25T16:08:34.565745Z","shell.execute_reply.started":"2021-08-25T16:08:34.459028Z","shell.execute_reply":"2021-08-25T16:08:34.564691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## t-SNE visualization of high-dimensional data\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset. press\n","metadata":{}},{"cell_type":"code","source":"m = TSNE(learning_rate=50)\ndf_numeric =trainessai.drop(['id','target'], axis=1).iloc[:3000,:]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(df_numeric)\n%timeit \nprint(tsne_features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:09:36.270532Z","iopub.execute_input":"2021-08-25T16:09:36.270895Z","iopub.status.idle":"2021-08-25T16:09:50.862426Z","shell.execute_reply.started":"2021-08-25T16:09:36.270865Z","shell.execute_reply":"2021-08-25T16:09:50.861467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataa=trainessai.drop(['id','target'], axis=1).iloc[:3000,:]\ndataa['x']=tsne_features[:, 0]\ndataa['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='target_binned', data=dataa)\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:09:50.866495Z","iopub.execute_input":"2021-08-25T16:09:50.866814Z","iopub.status.idle":"2021-08-25T16:09:51.307496Z","shell.execute_reply.started":"2021-08-25T16:09:50.866783Z","shell.execute_reply":"2021-08-25T16:09:51.306705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Target is really so disperssed**\n## PCA ","metadata":{}},{"cell_type":"code","source":"numdata=train[num_columns].copy()\nclus = KMeans(n_clusters=5, max_iter=2000)\nkmean_no_pca= clus.fit_predict(numdata)\ntrain1=train.copy()\ntrain1['cluster'] = kmean_no_pca\ntrain1['cluster'] = train1['cluster'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:11:16.491832Z","iopub.execute_input":"2021-08-25T16:11:16.492212Z","iopub.status.idle":"2021-08-25T16:11:22.75776Z","shell.execute_reply.started":"2021-08-25T16:11:16.492159Z","shell.execute_reply":"2021-08-25T16:11:22.75677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:11:28.250928Z","iopub.execute_input":"2021-08-25T16:11:28.251288Z","iopub.status.idle":"2021-08-25T16:11:28.285343Z","shell.execute_reply.started":"2021-08-25T16:11:28.251255Z","shell.execute_reply":"2021-08-25T16:11:28.284452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,26))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(7, 2, figure= fig, hspace= .2, wspace= .05)\nn =0\nfor i in range(7):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=train1, y='target', x='cont'+str(n), hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title('cont{}'.format(str(n)),fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n        \nfig.suptitle('Scatter plot of Target, Numerical and Cluster features', fontsize=20,y=.90)\nfig.text(0.11,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:11:41.586762Z","iopub.execute_input":"2021-08-25T16:11:41.5871Z","iopub.status.idle":"2021-08-25T16:14:18.293755Z","shell.execute_reply.started":"2021-08-25T16:11:41.587071Z","shell.execute_reply":"2021-08-25T16:14:18.292749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a series out of the Country column\ncluster = train1.cluster\n\n# Get the counts of each category\ncluster_counts = cluster.value_counts()\n\n# Print the count values for each category\nprint(cluster_counts)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:14:18.295134Z","iopub.execute_input":"2021-08-25T16:14:18.295421Z","iopub.status.idle":"2021-08-25T16:14:18.360774Z","shell.execute_reply.started":"2021-08-25T16:14:18.295394Z","shell.execute_reply":"2021-08-25T16:14:18.359857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nsns.barplot(cluster_counts.index,cluster_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of cluster')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('cluster', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:14:18.36214Z","iopub.execute_input":"2021-08-25T16:14:18.362411Z","iopub.status.idle":"2021-08-25T16:14:18.520702Z","shell.execute_reply.started":"2021-08-25T16:14:18.362385Z","shell.execute_reply":"2021-08-25T16:14:18.51987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Crosstab","metadata":{}},{"cell_type":"code","source":"import seaborn as sns \nred = sns.light_palette(\"red\", as_cmap=True)\ncross_tab=pd.crosstab(train1['cluster'], train1['cat1'], margins = True)\nH=cross_tab/cross_tab.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:15:54.385803Z","iopub.execute_input":"2021-08-25T16:15:54.386331Z","iopub.status.idle":"2021-08-25T16:15:54.587542Z","shell.execute_reply.started":"2021-08-25T16:15:54.3863Z","shell.execute_reply":"2021-08-25T16:15:54.586438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clustering seems like it wont add anything**\n","metadata":{}},{"cell_type":"markdown","source":"## Convert Dtype: ","metadata":{}},{"cell_type":"code","source":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:15:23.883135Z","iopub.execute_input":"2021-08-25T16:15:23.883513Z","iopub.status.idle":"2021-08-25T16:15:23.982765Z","shell.execute_reply.started":"2021-08-25T16:15:23.883471Z","shell.execute_reply":"2021-08-25T16:15:23.981715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the model features and target\n### Extract X and y ","metadata":{}},{"cell_type":"code","source":"# Create arrays for the features and the response variable\ny = train['target'].to_numpy()\nX = train.drop(['id','target'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:09.011164Z","iopub.execute_input":"2021-08-18T17:36:09.011748Z","iopub.status.idle":"2021-08-18T17:36:09.03013Z","shell.execute_reply.started":"2021-08-18T17:36:09.01164Z","shell.execute_reply":"2021-08-18T17:36:09.029123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create test and train groups\n\nNow we’ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We’ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","metadata":{}},{"cell_type":"code","source":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:11.77396Z","iopub.execute_input":"2021-08-18T17:36:11.774319Z","iopub.status.idle":"2021-08-18T17:36:11.86812Z","shell.execute_reply.started":"2021-08-18T17:36:11.774289Z","shell.execute_reply":"2021-08-18T17:36:11.866825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we’ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I’ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don’t want to include the target income column, so I’ve dropped that.\n### Cat Features ","metadata":{}},{"cell_type":"code","source":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:25:28.122742Z","iopub.execute_input":"2021-08-25T16:25:28.123123Z","iopub.status.idle":"2021-08-25T16:25:28.149223Z","shell.execute_reply.started":"2021-08-25T16:25:28.123091Z","shell.execute_reply":"2021-08-25T16:25:28.148158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Num Features ","metadata":{}},{"cell_type":"code","source":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:25:31.560489Z","iopub.execute_input":"2021-08-25T16:25:31.560846Z","iopub.status.idle":"2021-08-25T16:25:31.598409Z","shell.execute_reply.started":"2021-08-25T16:25:31.560817Z","shell.execute_reply":"2021-08-25T16:25:31.597407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:25:34.05872Z","iopub.execute_input":"2021-08-25T16:25:34.059108Z","iopub.status.idle":"2021-08-25T16:25:34.064681Z","shell.execute_reply.started":"2021-08-25T16:25:34.05907Z","shell.execute_reply":"2021-08-25T16:25:34.064002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## check that we have all column","metadata":{}},{"cell_type":"code","source":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling\n\n**Log Transformer**\n\nHelps with skewness No predetermined range for scaled data Useful only on non-zero, non-negative data\n\n**Min-Max Scaler**\n\nRescales to predetermined range [0–1] Doesn’t change distribution’s center (doesn’t correct skewness) Sensitive to outliers\n\n**Max Abs Scaler**\n\nRescales to predetermined range [-1–1] Doesn’t change distribution’s center Sensitive to outliers\n\n**Standard Scaler**\n\nShifts distribution’s mean to 0 & unit variance No predetermined range Best to use on data that is approximately normally distributed\n\n**Robust Scaler**\n\n0 mean & unit variance Use of quartile ranges makes this less sensitive to (a few) outliers No predetermined range\n\n**Power Transformer**\n\nHelps correct skewness 0 mean & unit variance No predetermined range Yeo-Johnson or Box-Cox Box-Cox can only be used on non-negative data\n\n**Cat encoding** LabelEncoder , OneHotEncoder ...ex \n\n### Preprocess: example ","metadata":{}},{"cell_type":"code","source":"# Import PowerTransformer\nfrom sklearn.preprocessing import PowerTransformer\nnumdata=train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64'])\n# Instantiate PowerTransformer\npow_trans = PowerTransformer()\n\n# Train the transform on the data\npow_trans.fit(numdata[['cont4']])\n\n# Apply the power transform to the data\nnumdata['cont4_LG'] = pow_trans.transform(numdata[['cont4']])\n\n# Plot the data before and after the transformation\nnumdata[['cont4', 'cont4_LG']].hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T11:08:49.743606Z","iopub.execute_input":"2021-08-18T11:08:49.743915Z","iopub.status.idle":"2021-08-18T11:08:50.621295Z","shell.execute_reply.started":"2021-08-18T11:08:49.743887Z","shell.execute_reply":"2021-08-18T11:08:50.620266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Outlier Handling ","metadata":{}},{"cell_type":"code","source":"# Outlier Handle \nclass OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select best preprocess pipe :\n\nthis job was done in this notebook : \n\nhttps://www.kaggle.com/bannourchaker/1-2eda-selectbestpreprocespipedataxgbr\n>      encoder \t     scaler        \trmse\n\n        0 \tLeaveOneOutEncoder \tStandardScaler \t0.754493\n        6 \tLeaveOneOutEncoder \tMaxAbsScaler \t0.754493\n        3 \tLeaveOneOutEncoder \tNormalizer \t0.763461\n        1 \tMEstimateEncoder \tStandardScaler \t0.820263\n        7 \tMEstimateEncoder \tMaxAbsScaler \t0.820263","metadata":{}},{"cell_type":"markdown","source":"\n## Feature Engineering\n\n### Sparse Interactiions \n","metadata":{}},{"cell_type":"code","source":"class SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n            \n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n            \n        spi = self._create_sparse_interactions(X)\n        return spi\n    \n    \n    def get_feature_names(self):\n        return self.feature_names\n    \n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n        \n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n                \n                # get column multiplications value\n                out = X[:, col_ixs[0]]    \n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimensionality reduction\n### Feature Selection\n\n\nFeature selection is a method of selecting features from your feature set to be used for modeling. It draws from a set of existing features, so it's different than feature engineering because it doesn't create new features. The overarching goal of feature selection is to improve your model's performance. Perhaps your existing feature set is much too large, or some of the features you're working with are unnecessary. There are different ways you can perform feature selection. It's possible to do it in an automated way. Scikit-learn has several methods for automated feature selection, such as choosing a variance threshold and using univariate statistical tests\n\nFeature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n\nStatistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the choice of statistical measures depends on the data type of both the input and output variables.\n\nAs such, it can be challenging for a machine learning practitioner to select an appropriate statistical measure for a dataset when performing filter-based feature selection.\n\nIn this post, you will discover how to choose statistical measures for filter-based feature selection with numerical and categorical data. We can summarize feature selection as follows.\n\n    Feature Selection: Select a subset of input features from the dataset.\n    \n        Unsupervised: Do not use the target variable (e.g. remove redundant variables).\n        \n            Correlation\n            \n        Supervised: Use the target variable (e.g. remove irrelevant variables).\n        \n            Wrapper: Search for well-performing subsets of features.\n                RFE\n                \n            Filter: Select subsets of features based on their relationship with the target.\n                Statistical Methods\n                \n                Feature Importance Methods\n                \n            Intrinsic: Algorithms that perform automatic feature selection during training.\n                Decision Trees\n                \n    Dimensionality Reduction: Project input data into a lower-dimensional feature space.\n\n\nThis is done in this notebook : \n\nhttps://www.kaggle.com/bannourchaker/1-featuresengineer-selectionpart1?scriptVersionId=72906910","metadata":{}},{"cell_type":"markdown","source":"### Preprocess Cat that occur rarely \n#### What is high cardinality?\n\nAlmost all datasets now have categorical variables. Each categorical variable consists of unique values. A categorical feature is said to possess high cardinality when there are too many of these unique values. One-Hot Encoding becomes a big problem in such a case since we have a separate column for each unique value (indicating its presence or absence) in the categorical variable. This leads to two problems, one is obviously space consumption, but this is not as big a problem as the second problem, the curse of dimensionality.\n#### The Curse of Dimensionality\n\nHere is a simple summarization:\n\n    As the number of features grows, the amount of data we need to accurately be able to distinguish between these features (in order to give us a prediction) and generalize our model (learned function) grows EXPONENTIALLY.   \nwould like to use Yoshua Bengio’s (Yes the legendary Yoshua Bengio !) quora answer to explain this in more detail. I strongly advise reading the whole answer here. According to the answer, increasing the number of different values in a feature simply increases the total number of possible combinations that can be made using the input row (containing n such features). Say we have two features with two distinct values each, this gives us a total of 4 possible ways to combine the two features. Now if one of these had three distinct values we would have 3X2 =6 possible ways to combine them.\n\nIn classical non-parametric learning algorithms (e.g. nearest-neighbor, Gaussian kernel SVM, Gaussian kernel Gaussian Process, etc.) the model needs to see at least one example for each of these combinations (or at least as many as necessary to cover all the variations of configurations of interest), in order to produce a correct answer, one that is different from the target value required for other nearby configurations.\n\nThere is a workaround to this, that is the model even in the absence of a lot of training data can discern between configurations (not in the training set) for future predictions provided there is some sort of structure (pattern) in these combinations. In most cases, high cardinality makes it difficult for the model to identify such patterns and hence the model doesn’t generalise well to examples outside the training set.    \n####  Reducing Cardinality by using a simple Aggregating function\n\nBelow is a simple function I use to reduce the cardinality of a feature. The idea is very simple. Leave instances belonging to a value with high frequency as they are and replace the other instances with a new category which we will call other.\n\n    Choose a threshold\n    Sort unique values in the column by their frequency in descending order\n    Keep adding the frequency of these sorted (descending) unique values until a threshold is reached.\n    These are the unique categories we will keep and instances of all other categories shall be replaced by “other”.\nLet’s run through a quick example before going through the code. Say our column colour has 100 values and our threshold is 90% (that is 90). We have 5 different categories of colours: Red (50), Blue(40), Yellow (5), Green (3) and Orange (2). The numbers within the bracket indicate how many instances of that category are present in the column.\n\nWe see that Red (50)+Blue (40) reaches our threshold of 90. In that case, we retain only 2 categories (Red, Blue) and mark all other instances of other colours as “Other”\nThus we have reduced cardinality from 5 to 3 (Red, Blue, Other)\n\nHere is the utility function I wrote to facilitate this. It’s well commented and follows exactly what I described above so you won’t have a problem following along. We can set a custom threshold and the return_categories option optionally lets us see the list of all unique values after reducing cardinality.\n    ","metadata":{}},{"cell_type":"code","source":"# Create a series out of the Country column\ncat6 = train.cat6\n\n# Get the counts of each category\ncat6_counts = cat6.value_counts()\n\n# Print the count values for each category\nprint(cat6_counts)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:41:58.852107Z","iopub.execute_input":"2021-08-18T17:41:58.852452Z","iopub.status.idle":"2021-08-18T17:41:58.862122Z","shell.execute_reply.started":"2021-08-18T17:41:58.852421Z","shell.execute_reply":"2021-08-18T17:41:58.86099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nsns.barplot(cat6_counts.index, cat6_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of countries')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Country', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T13:25:42.311005Z","iopub.execute_input":"2021-08-18T13:25:42.31164Z","iopub.status.idle":"2021-08-18T13:25:42.493688Z","shell.execute_reply.started":"2021-08-18T13:25:42.311603Z","shell.execute_reply":"2021-08-18T13:25:42.493014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train['cat7'].astype('category').cat.categories.tolist()\ncounts = train['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T13:25:51.649916Z","iopub.execute_input":"2021-08-18T13:25:51.650262Z","iopub.status.idle":"2021-08-18T13:25:51.85131Z","shell.execute_reply.started":"2021-08-18T13:25:51.650232Z","shell.execute_reply":"2021-08-18T13:25:51.85029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_columns :\n    cat_frenquency = (train[col].value_counts())/train.shape[0]\n    botton_decile = cat_frenquency.quantile(q=0.1)\n    print(col , cat_frenquency,botton_decile )   ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T13:25:59.256973Z","iopub.execute_input":"2021-08-18T13:25:59.257363Z","iopub.status.idle":"2021-08-18T13:25:59.306379Z","shell.execute_reply.started":"2021-08-18T13:25:59.25733Z","shell.execute_reply":"2021-08-18T13:25:59.305565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def condense_category(col, min_freq=0.1, new_name='other'):\n    series = pd.value_counts(col)\n    mask = (series/series.sum()).lt(min_freq)\n    return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\ntrain_condense=train.copy()\ntrain_condense[cat_columns]=train_condense[cat_columns].apply(condense_category, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:39.17379Z","iopub.execute_input":"2021-08-18T17:36:39.174394Z","iopub.status.idle":"2021-08-18T17:36:39.401058Z","shell.execute_reply.started":"2021-08-18T17:36:39.174304Z","shell.execute_reply":"2021-08-18T17:36:39.400171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_condense.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T13:41:46.507768Z","iopub.execute_input":"2021-08-18T13:41:46.508302Z","iopub.status.idle":"2021-08-18T13:41:46.649561Z","shell.execute_reply.started":"2021-08-18T13:41:46.508253Z","shell.execute_reply":"2021-08-18T13:41:46.648813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert Ddtype","metadata":{}},{"cell_type":"code","source":"train_condense[train_condense.select_dtypes(['float64']).columns] = train_condense[train_condense.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain_condense[train_condense.select_dtypes(['object']).columns] = train_condense.select_dtypes(['object']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:44.692087Z","iopub.execute_input":"2021-08-18T17:36:44.692647Z","iopub.status.idle":"2021-08-18T17:36:45.164463Z","shell.execute_reply.started":"2021-08-18T17:36:44.69259Z","shell.execute_reply":"2021-08-18T17:36:45.163329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_columns :\n    cat_frenquency = (train_condense[col].value_counts())\n    botton_decile = cat_frenquency.quantile(q=0.1)\n    print(col , cat_frenquency,botton_decile )   ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T13:44:16.216269Z","iopub.execute_input":"2021-08-18T13:44:16.216599Z","iopub.status.idle":"2021-08-18T13:44:16.257611Z","shell.execute_reply.started":"2021-08-18T13:44:16.216571Z","shell.execute_reply":"2021-08-18T13:44:16.256913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_condense['cat7'].astype('category').cat.categories.tolist()\ncounts = train_condense['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T13:44:24.038562Z","iopub.execute_input":"2021-08-18T13:44:24.038932Z","iopub.status.idle":"2021-08-18T13:44:24.150707Z","shell.execute_reply.started":"2021-08-18T13:44:24.038897Z","shell.execute_reply":"2021-08-18T13:44:24.150044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# X and Y for data Condense ","metadata":{}},{"cell_type":"code","source":"# Create arrays for the features and the response variable\ny_condense = train_condense['target'].to_numpy()\nX_condense = train_condense.drop(['id','target'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train_condense , X_test_condense , y_train_condense , y_test_condense  = train_test_split(X_condense , y_condense , test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test_condense.shape[0], X_train_condense.shape[0], X_test_condense.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:54:13.55126Z","iopub.execute_input":"2021-08-18T17:54:13.551764Z","iopub.status.idle":"2021-08-18T17:54:13.647298Z","shell.execute_reply.started":"2021-08-18T17:54:13.55172Z","shell.execute_reply":"2021-08-18T17:54:13.646105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipe Cat ","metadata":{}},{"cell_type":"code","source":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_encoder1 = make_pipeline(\n    SimpleImputer(strategy='most_frequent', add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_LabelEncoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    LabelEncoder()\n)\nfill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder()\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:37:06.656747Z","iopub.execute_input":"2021-08-18T17:37:06.657219Z","iopub.status.idle":"2021-08-18T17:37:06.664542Z","shell.execute_reply.started":"2021-08-18T17:37:06.657181Z","shell.execute_reply":"2021-08-18T17:37:06.663376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num Features :","metadata":{}},{"cell_type":"code","source":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_encoder1 = make_pipeline(\n    SimpleImputer(strategy='most_frequent', add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_LabelEncoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    LabelEncoder()\n)\nfill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder()\n)\nfill_missing_then_StandardScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\nfill_missing_then_StandardScalerPolynomialFeatures = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n     PolynomialFeatures(degree=2),StandardScaler()\n)\nfill_missing_then_RobustScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    RobustScaler()\n)\nfill_missing_then_MinMaxScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    MinMaxScaler()\n)\nfill_missing_then_Outlier_MinMax = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    MinMaxScaler()\n) ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:42:13.055207Z","iopub.execute_input":"2021-08-18T17:42:13.055702Z","iopub.status.idle":"2021-08-18T17:42:13.063053Z","shell.execute_reply.started":"2021-08-18T17:42:13.055664Z","shell.execute_reply":"2021-08-18T17:42:13.062153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA Cleaning and feature Engineering\nExploring Clustering and PCA\nMainly there are two way to do this\n\n    First PCA and then clustering\n    Clustering and then doing PCA\n\nIt also worth mentioning that there is two ways of using clusters as features\n\n    using cluster label (in short one columns with each number representing the cluster\"\n    using clusters labels ( adding k number of columns representing the distance to the centroid of each cluster\"\n\nFirst we do Clustring \"using num_col only\"","metadata":{}},{"cell_type":"code","source":"clus = KMeans(n_clusters=5, max_iter=2000)\nkmean_no_pca= clus.fit_predict(train[num_col])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add columns with kmeans, assuming input already scaled\nclass KmeansTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, num_clusters = 4):\n        self.num_clusters = num_clusters\n        if self.num_clusters > 0:\n            self.kmeans = KMeans(n_clusters=self.num_clusters, n_init=3, random_state=0)\n    \n    def fit(self, X, y=None):\n        if self.num_clusters > 0:\n            self.kmeans.fit(X)\n        return self\n    \n    def transform(self, X, y=None):\n        if self.num_clusters > 0:\n            Xkf = self.kmeans.transform(X)\n            df = pd.DataFrame()\n            for i in range(self.num_clusters):\n                df[f\"Centroid_{i}\"] = Xkf[:,i]\n            return df\n        else: # if no kmeans requested, return first column unmodified as answer\n            # need to return a one column dataframe rather than series\n            # in order for FeatureUnion to work correctly\n            tf = np.zeros(X.columns.size,dtype=bool)\n            tf[0] = True\n            return X.loc[:,tf]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Modeling\n# Steps for K-fold cross-validation\n\n    Split the dataset into K equal partitions (or \"folds\")\n        So if k = 5 and dataset has 150 observations\n        Each of the 5 folds would have 30 observations\n    Use fold 1 as the testing set and the union of the other folds as the training set\n        Testing set = 30 observations (fold 1)\n        Training set = 120 observations (folds 2-5)\n    Calculate testing accuracy\n    Repeat steps 2 and 3 K times, using a different fold as the testing set each time\n        We will repeat the process 5 times\n        2nd iteration\n            fold 2 would be the testing set\n            union of fold 1, 3, 4, and 5 would be the training set\n        3rd iteration\n            fold 3 would be the testing set\n            union of fold 1, 2, 4, and 5 would be the training set\n        And so on...\n    Use the average testing accuracy as the estimate of out-of-sample accuracy\n\nDiagram of 5-fold cross-validation\n\n![image.png](data:image/png;base64,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)\n\n\n\n\n\n\n# Comparing cross-validation to train/test split\n\nAdvantages of cross-validation:\n\n    More accurate estimate of out-of-sample accuracy\n    More \"efficient\" use of data\n        This is because every observation is used for both training and testing\n\nAdvantages of train/test split:\n\n    Runs K times faster than K-fold cross-validation\n        This is because K-fold cross-validation repeats the train/test split K-times\n    Simpler to examine the detailed results of the testing process\n\n# Cross-validation recommendations\n\n    K can be any number, but K=10 is generally recommended\n        This has been shown experimentally to produce the best out-of-sample estimate\n    For classification problems, stratified sampling is recommended for creating the folds\n        Each response class should be represented with equal proportions in each of the K folds\n            If dataset has 2 response classes\n                Spam/Ham\n                20% observation = ham\n                Each cross-validation fold should consist of exactly 20% ham\n        scikit-learn's cross_val_score function does this by default\n\n# Feature engineering and selection within cross-validation iterations\n\n    Normally, feature engineering and selection occurs before cross-validation\n    Instead, perform all feature engineering and selection within each cross-validation iteration\n    More reliable estimate of out-of-sample performance since it better mimics the application of the model to out-of-sample data\n      \n\n\n## Step 1: Cross-Validation","metadata":{}},{"cell_type":"code","source":"cross_validation_design = KFold(n_splits=5,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:42:27.93444Z","iopub.execute_input":"2021-08-18T17:42:27.934979Z","iopub.status.idle":"2021-08-18T17:42:27.940532Z","shell.execute_reply.started":"2021-08-18T17:42:27.934931Z","shell.execute_reply":"2021-08-18T17:42:27.939857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Catboost / Xgboost / Lgbm\n## Define Baseline XGBR ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\nXGBR = XGBRegressor(**xgb_params,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\"\n                   )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compose num+cat : ColumnTransformer\n## Multiple  preprocess + Features engnieer + Features Selections ","metadata":{}},{"cell_type":"code","source":"drop_colonnes=[]\npassthrough_colonnes=[]\n# encode + featres engineers \nfill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder(),\n    SparseInteractions(degree=2)\n)\n# Polynominal features + \nfill_missing_then_StandardScalerPolynomialFeatures = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n     PolynomialFeatures(degree=2),StandardScaler()\n)\n# Complete pipe \ndata_preprocessOrdinalEncoderSparseStandardScalerPolynomial = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n# Standar preprocess pipes \ndata_preprocessOrdinalEncoder = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\ndata_preprocessone_hot_encoder = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n\n\n# Pipe 6:OrdinalEncoderSparse /StandardScalerPolynomial without reduction\n# 0.7048661500017998/Public Score : 0.71829\npipepolyOrdiSparseStandarPolywithoutreduction = Pipeline([\n        ('scaler+polyfornum+sparseforcat', data_preprocessOrdinalEncoderSparseStandardScalerPolynomial),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n\n# ADD poly num features : 0.6949615269482926/Public Score :0.72104\ndata_preprocessoneOnehotPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)\n# Pipe ONE with PolynomialFeatures: 0.6949615269482926 /Public Score :0.72104\npipeONEPoly = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n        ('classifier', XGBR)])\n\n\n#  Pipe Ordinal encoder + standar/poly for num\n#  0.6948632925742044/Public score : 0.72138\npipepolyordi = Pipeline([\n        ('scaler+ordi+poly', data_preprocessOrdinalEncoderPolynomialFeatures),\n        ('classifier', XGBR)])\n\n# Pipe : poly for num +ORDINAL sparse for cat and union:Reduce f_regression + interactions\n#0.7011630368570897 /Public score : 0.72686\npipepolyOrdiSparseStandarPolyf_regression = Pipeline([\n        ('scaler+polyfornum+sparseforcat', data_preprocessOrdinalEncoderSparseStandardScalerPolynomial),\n         ('dim_red', SelectKBest(f_regression, k=20)),\n         ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n\n# 21Pipe add sparse interaction after features union \n#+reduction f_regression without interactions between features\n# 0.7202125250928905/Public Score:0.73536\npipepolyInteractionf_regression21 = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n         ('dim_red', SelectKBest(f_regression, k=20)),\n         #('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n\n# Pipe add sparse interaction after features union +reduction f_regression\n# 0.7083904092283486/ Public Score 0.73656\npipepolyInteractionf_regression = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n         ('dim_red', SelectKBest(f_regression, k=20)),\n         ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n\n# pipe OHE  with redcution f_classif : 0.7316258211241139 /Public score :0.74741\npipeONEPolyInteractionreduceclassif = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n         ('dim_red', SelectKBest(f_classif, k=20)),\n         ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n# PCA 2 without interactions :\n# 0.7416545727730322 /Public Score : 0.74838\npipepolypca2 = Pipeline([\n        ('scaler+ordisparse+poly', data_preprocessOrdinalEncoderSparseStandardScalerPolynomial),\n        ('reducer', PCA(n_components=0.8)),\n        #('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n# Pipe With PCA \n#0.7396454948292566 /Public Score 0.74868\npipepolypca = Pipeline([\n        ('scaler+ordisparse+poly', data_preprocessOrdinalEncoderSparseStandardScalerPolynomial),\n        ('reducer', PCA(n_components=0.8)),\n        ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n# New pipe to try :\npipepolyInteractionmutual_info_classif = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n         ('dim_red', SelectKBest(mutual_info_regression, k=20)),\n         ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_encoder1 = make_pipeline(\n    SimpleImputer(strategy='most_frequent', add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_LabelEncoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    LabelEncoder()\n)\nfill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder()\n)\nfill_missing_then_StandardScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\nfill_missing_then_StandardScalerPolynomialFeatures = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n     PolynomialFeatures(degree=2),StandardScaler()\n)\nfill_missing_then_RobustScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    RobustScaler()\n)\nfill_missing_then_MinMaxScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    MinMaxScaler()\n)\nfill_missing_then_Outlier_MinMax = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    MinMaxScaler()\n) \nfill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_encoder1 = make_pipeline(\n    SimpleImputer(strategy='most_frequent', add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_LabelEncoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    LabelEncoder()\n)\nfill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder()\n)\ndrop_colonnes=[]\npassthrough_colonnes=[]\n\n# encode + featres engineers \nfill_missing_then_OrdinalEncoderSparse = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder(),\n    SparseInteractions(degree=2)\n)\n\n# Polynominal Robust + Robust\nfill_missing_then_RobustScalerPolynomialFeatures =make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n     PolynomialFeatures(degree=2),RobustScaler()\n)\n\n# Polynominal features + Standar\nfill_missing_then_StandardScalerPolynomialFeatures = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n     PolynomialFeatures(degree=2),StandardScaler()\n)\n\n# Complete pipe Standar \ndata_preprocessOrdinalEncoderSparseStandardScalerPolynomial = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoderSparse , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n\n# Complete pipe Robust \ndata_preprocessOrdinalEncoderSparseRobustscalerPolynomial = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoderSparse , cat_columns),\n    ( fill_missing_then_RobustScalerPolynomialFeatures, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n# Complete pipe \ndata_preprocessOrdinalEncoderSparseStandardScalerPolynomial = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoderSparse , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n# Standar preprocess pipes \ndata_preprocessOrdinalEncoder = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\ndata_preprocessone_hot_encoder = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n\n\n\n\ndata_preprocessOrdinalEncoderPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)\ndata_preprocessoneOnehotPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)\n\n# Pipe 6:OrdinalEncoderSparse /StandardScalerPolynomial without reduction\n# 0.7048661500017998/Public Score : 0.71829\nFirstpipepolyOrdiSparseStandarPolywithoutreduction = Pipeline([\n        ('scaler+polyfornum+sparseforcat', data_preprocessOrdinalEncoderSparseStandardScalerPolynomial),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n# to try :\nSecondpipepolyOrdiSparseStandarPolywithoutreduction = Pipeline([\n        ('scaler+polyfornum+sparseforcat', data_preprocessOrdinalEncoderSparseRobustscalerPolynomial),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n# ADD poly num features : 0.6949615269482926/Public Score :0.72104\ndata_preprocessoneOnehotPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)\n# Pipe ONE with PolynomialFeatures: 0.6949615269482926 /Public Score :0.72104\npipeONEPoly = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n        ('classifier', XGBR)])\n\n\n#  Pipe Ordinal encoder + standar/poly for num\n#  0.6948632925742044/Public score : 0.72138\npipepolyordi = Pipeline([\n        ('scaler+ordi+poly', data_preprocessOrdinalEncoderPolynomialFeatures),\n        ('classifier', XGBR)])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse1 =cross_val_score(SecondpipepolyOrdiSparseStandarPolywithoutreduction, X[0:300], y[0:300], cv=cross_validation_design,scoring='neg_root_mean_squared_error').mean()\nprint()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nrmse1 =cross_val_score(FirstpipepolyOrdiSparseStandarPolywithoutreduction, X1, y1, cv=cross_validation_design,scoring='neg_root_mean_squared_error').mean()\nprint()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning model with Optuna\n## Using Optuna:\n\nSteps-\n\n    Define objective function to be optimized.Here run.\n    Suggest hyperparameter values using trial object.\n    Create a study object and invoke the optimize method over 100 trials.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    \n    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 5000)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n    gamma = trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.1)\n    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 7, step=2)\n    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1)\n    reg_alpha = trial.suggest_float(\"reg_alpha\", 0., 1.0, step=0.1)\n    reg_lambda = trial.suggest_float(\"reg_lambda\", 0., 1.0, step=0.1)\n    \n    \n    model = XGBRegressor(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        min_child_weight=min_child_weight,\n        colsample_bytree=colsample_bytree,\n        subsample=subsample,\n        reg_alpha=reg_alpha,\n        reg_lambda=reg_lambda,\n        n_jobs=-1, \n        tree_method='gpu_hist', \n        gpu_id=0\n    )\n    \n    model.fit(X_train, y_train)\n    \n    y_hat = model.predict(X_valid)\n    \n    return mean_squared_error(y_valid, y_hat, squared=False)\n\n\nstudy.optimize(objective, n_trials=300)\nstudy = optuna.create_study(direction='minimize')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    {'n_estimators': 2694,\n     'max_depth': 3,\n     'learning_rate': 0.07201051864225298,\n     'gamma': 0.5,\n     'min_child_weight': 3,\n     'subsample': 0.8,\n     'colsample_bytree': 0.5,\n     'reg_alpha': 0.30000000000000004,\n     'reg_lambda': 0.30000000000000004}\n\n     Number of finished trials: 100\n    Best trial:\n    Value: 0.7174799683646991\n    Params: \n        learning_rate: 0.07977009001145592\n        reg_lambda: 0.1555231584050028\n        reg_alpha: 34.656377999143324\n        subsample: 0.9236727428353445\n        colsample_bytree: 0.12271405698528293\n        max_depth: 2","metadata":{}},{"cell_type":"code","source":"print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: {}\".format(trial.value))\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" study.best_params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbest_params = study.best_params\nxgb_reg_final = XGBRegressor(tree_method='gpu_hist', \n                                 gpu_id=0, \n                                 **best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning with GridSearch ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nXGBR = XGBRegressor()\nXGBR_MODEL = {}\n# Définir la pipeline\nXGBR_MODEL['pipeline'] = Pipeline([\n                                  ('data_process', data_preprocess),\n                                  ('XGBR', XGBR)\n                                  ])\n\n# Définir la grille\nXGBR_MODEL['hyperparams'] = {}\nXGBR_MODEL['hyperparams']['XGBR__n_estimators'] = [100,200,2694,4575,5000,9489,80000]\n#XGBR_MODEL['hyperparams']['XGBR__colsample_bytree'] = [0.5,0.7, 0.8,0.17]\n#XGBR_MODEL['hyperparams']['XGBR__max_depth'] = [5,7,15,2,3]\n#XGBR_MODEL['hyperparams']['XGBR__reg_alpha'] = [1.1, 1.2,1.5,0.3,40.12 ]\n#XGBR_MODEL['hyperparams']['XGBR__reg_lambda'] = [1.1, 1.2, 1.3,38,0.3,67.79,]\n#XGBR_MODEL['hyperparams']['XGBR__subsample'] = [0.7, 0.8, 0.9,0.955]\nXGBR_MODEL['hyperparams']['XGBR__gamma '] = [1,0.3,0.5]\nXGBR_MODEL['hyperparams']['XGBR__learning_rate'] = [ 0.039,0.05,0.072,0.17]\nXGBR_MODEL['hyperparams']['XGBR__min_child_weight'] = [3,155]\nXGBR_MODEL['hyperparams']['XGBR__booster '] = ['gbtree']\n# Effectuer la GridSearch\nXGBR_MODEL['gridsearch'] = GridSearchCV(\n    estimator=XGBR_MODEL['pipeline'],\n    param_grid=XGBR_MODEL['hyperparams'],\n    cv=cross_validation_design,\n    scoring='neg_root_mean_squared_error'\n    )\n#Define SVR classifier\nXGBR_MODEL['gridsearch'].fit(X_train, y_train)\nXGBR_accuracy = XGBR_MODEL['gridsearch'].score(X_test, y_test)\nprint('SXGBR Accuracy : ', XGBR_accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T11:28:13.024694Z","iopub.execute_input":"2021-08-18T11:28:13.025027Z","iopub.status.idle":"2021-08-18T11:45:07.087103Z","shell.execute_reply.started":"2021-08-18T11:28:13.025Z","shell.execute_reply":"2021-08-18T11:45:07.086022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGBR_MODEL['gridsearch'].cv_results_","metadata":{"execution":{"iopub.status.busy":"2021-08-18T12:30:55.22788Z","iopub.execute_input":"2021-08-18T12:30:55.228234Z","iopub.status.idle":"2021-08-18T12:30:55.23998Z","shell.execute_reply.started":"2021-08-18T12:30:55.228191Z","shell.execute_reply":"2021-08-18T12:30:55.238938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGBR_MODEL['gridsearch'].best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-18T12:32:34.406374Z","iopub.execute_input":"2021-08-18T12:32:34.406926Z","iopub.status.idle":"2021-08-18T12:32:34.413196Z","shell.execute_reply.started":"2021-08-18T12:32:34.406881Z","shell.execute_reply":"2021-08-18T12:32:34.412578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try Crossvalidation for prediction ","metadata":{}},{"cell_type":"code","source":"def train(model):\n    \n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                       ('model', model)\n                     ])\n\n    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n    ypred = 0\n    total_loss = 0\n    for train_indx, test_indx in cv.split(X):\n        X_train, X_val = X.iloc[train_indx], X.iloc[test_indx]\n        y_train, y_val = y.iloc[train_indx], y.iloc[test_indx]\n        clf.fit(X_train, y_train)\n        \n        yhat = clf.predict(X_val)\n        score = mean_squared_error(yhat, y_val, squared=False)\n        print(f\"Loss:{score}\")\n        ypred += clf.predict(X_test) / 10\n        total_loss += score / 10\n        \n    print(f\"Avg. Loss: {total_loss}\")     \n    return ypred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBRegressor(n_estimators= 10000, booster='gbtree', tree_method='gpu_hist', \n                    learning_rate= 0.35, subsample= 0.926, max_depth= 2,\n                    colsample_bytree = 0.84, reg_alpha = 34.9, random_state = 42,\n                    reg_lambda = 35.1, n_jobs=-1)\n\nfinal_prediction = train(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select Best Model and retrain on all data ","metadata":{}},{"cell_type":"code","source":"data_cleaning = make_column_transformer(\n    ( OneHotEncoder(), colonnes_catégoriques ),\n    ( fill_missing_then_one_hot_encoder , colonnes_catégoriques_avec_valeurs_manquantes),\n    ( SimpleImputer(strategy='mean'), colonnes_numériques_avec_valeurs_manquantes),\n    ( CountVectorizer(), 'Name'),\n    ( extraire_lettre_cabine, 'Cabin'),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n\nmodel_final = Pipeline([('data_cleaning', data_cleaning),\n                        ('rf', RandomForest(random_state=7,\n                                            max_depth=10,\n                                            max_features=0.25,\n                                            n_estimators=150))\n                        ])\n\n\n# on fit la meilleur pipe sur toute nos données de train\nmodel_final.fit(X, Y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","metadata":{}},{"cell_type":"code","source":"pipepolypca.fit(X, y)\npreds_valid = pipepolypca.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))\ntest_final= test.drop(['id'], axis=1)\n# Use the model to generate predictions\npredictions = pipepolypca.predict(test_final)\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,'target': predictions})\noutput.to_csv('6submissionpipepolypca.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References : \n# Pipe and code to follow :\n### Xgboost :  \nhttps://www.kaggle.com/tilii7/bias-correction-xgboost\n\nhttps://www.kaggle.com/ranjeetshrivastav/optuna-voting-xgb-lgb\n\nhttps://www.kaggle.com/haotianhang/30-days-of-ml-xgboost-optuna-best-score-0-71949\n\nhttps://www.kaggle.com/mohammadghanaym/eda-xgboost-hyperparameters-tuning\n\nhttps://www.kaggle.com/maximkazantsev/30dml-eda-xgboost\n\nhttps://www.kaggle.com/andersvp/xgboost-kfold-and-hyperopt-tuning\n\nhttps://www.kaggle.com/mhmdsyed/30-day-ml-starter-eda-simple-xgboost-model\n\nhttps://www.kaggle.com/miguelquiceno/30-days-xgboost-optuna\n\nhttps://www.kaggle.com/pranav2109/xgbpipeline\n\nhttps://www.kaggle.com/afifnugraha/kaggle-30-days-ml-competition-by-afif\n\nhttps://www.kaggle.com/kavehshahhosseini/30-days-ml-xgboost-hyperparameter-optimization\n\nhttps://www.kaggle.com/kavehshahhosseini/30-days-ml-xgboost-hyperparameter-optimization\n\nhttps://www.kaggle.com/mrkyawswarthant/simple-xgboost-with-gpu\n\nhttps://www.kaggle.com/miguelquiceno/30-days-kfold-xgboost\n\nhttps://www.kaggle.com/linzino/use-easily-with-xgboost-from-getting-started\nhttps://www.kaggle.com/yogidsba/30days-of-ml-xgboost\n\nhttps://www.kaggle.com/boneacrabonjac/30-days-ml-xgbr-with-auto-eda\n\nhttps://www.kaggle.com/rishirajacharya/30-days-xgboost-with-5-folds-eda-gpu\n\nhttps://www.kaggle.com/mohammadghanaym/30-ml-competition-xgboost\n\nhttps://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-xgboost\n\nhttps://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-catboost\n\nhttps://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm\n\nhttps://www.kaggle.com/medicinewave/1stday-easy-lgbm-tuned\n\nhttps://www.kaggle.com/pranav2109/featureengineering-ensemble-xgbpipeline\n\nhttps://www.kaggle.com/heyrobin/30-days-ml-with-xgbr\n\nhttps://www.kaggle.com/mohammadghanaym/eda-xgboost-hyperparameters-tuning\n\nhttps://www.kaggle.com/hamzaghanmi/30-days-of-ml-competition-step-by-step\n\n### LGBM\nhttps://www.kaggle.com/yaeulrichgaba/trying-lgbm-simple-30-days-ml-competition\n\nhttps://www.kaggle.com/seraphwedd18/lightgbm-with-feature-engineering\n\nhttps://www.kaggle.com/jahaziel/lgb-optuna\n\nhttps://www.kaggle.com/kokitanisaka/lightgbm-starter-30-days-of-ml\n\nhttps://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-xgboost\n\n# cat boost :\nhttps://www.kaggle.com/maximkazantsev/30dml-eda-simple-catboost\n\nhttps://www.kaggle.com/aymanlafaz/30dml-catboost-optuna-optim\n\n# Features Engineer :\nhttps://www.kaggle.com/abhishek/competition-part-2-feature-engineering\n\nhttps://www.kaggle.com/hongpeiyi/tuning-xgboost-with-optuna\n\n# Deep Learning : \nhttps://www.kaggle.com/yassinemesbahi/eda-fe-artificial-neural-network-model\n\nhttps://www.kaggle.com/sgedela/30-days-of-ml-competition\n\nhttps://www.kaggle.com/tsubajiro/prediction-with-neural-network\n\nhttps://www.kaggle.com/mtinti/keras-starter-with-bagging-1111-84364\n\n# AutoML\n# Ensemble : \n\nhttps://www.kaggle.com/noulan/standing-on-the-shoulders-of-giants-ensemble\n\n\nhttps://www.kaggle.com/kokitanisaka/a-way-of-finding-optimum-blending-weight\n\n\nhttps://www.kaggle.com/ranjeetshrivastav/optuna-voting-xgb-lgb\n\n\nhttps://www.kaggle.com/dwin183287/30-days-of-ml-eda\n\n\nhttps://www.kaggle.com/vipin20/getting-start-30-days-ml\n\nMutal information is used for non linear relation :\n\nhttps://www.kaggle.com/pranav2109/featureengineering-ensemble-xgbpipeline\n\nhttps://www.kaggle.com/sgedela/30-days-of-ml-competition    \nhttps://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n\nhttps://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n# Run on gpu using cuml: \nhttps://www.kaggle.com/jonaspalucibarbosa/30days-random-forest-on-gpu-rapids","metadata":{}}]}