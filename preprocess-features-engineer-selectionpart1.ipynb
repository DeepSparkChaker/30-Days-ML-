{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Import helpful libraries","metadata":{}},{"cell_type":"code","source":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nfrom scipy import sparse\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nfrom sklearn.manifold import TSNE\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T11:42:08.244820Z","iopub.execute_input":"2021-08-20T11:42:08.245253Z","iopub.status.idle":"2021-08-20T11:42:14.718409Z","shell.execute_reply.started":"2021-08-20T11:42:08.245214Z","shell.execute_reply":"2021-08-20T11:42:14.715817Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-128372dedd08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m  \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmirrored_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmirrored_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_worker_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFConfigClusterResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/cluster_resolver/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnionClusterResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgce_cluster_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCEClusterResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkubernetes_cluster_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKubernetesClusterResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslurm_cluster_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSlurmClusterResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfconfig_cluster_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFConfigClusterResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0m_KUBERNETES_API_CLIENT_INSTALLED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mk8sclient\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mk8sconfig\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kubernetes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"17.17.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kubernetes/client/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# import apis into sdk package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madmissionregistration_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdmissionregistrationApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madmissionregistration_v1_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdmissionregistrationV1Api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madmissionregistration_v1beta1_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdmissionregistrationV1beta1Api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kubernetes/client/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import apis into api package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madmissionregistration_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdmissionregistrationApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madmissionregistration_v1_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdmissionregistrationV1Api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madmissionregistration_v1beta1_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdmissionregistrationV1beta1Api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kubernetes/client/api/admissionregistration_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApiClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m from kubernetes.client.exceptions import (  # noqa: F401\n\u001b[1;32m     22\u001b[0m     \u001b[0mApiTypeError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApiValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kubernetes/client/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1_replication_controller_status\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV1ReplicationControllerStatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1_resource_attributes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV1ResourceAttributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1_resource_field_selector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV1ResourceFieldSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1_resource_quota\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV1ResourceQuota\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkubernetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1_resource_quota_list\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV1ResourceQuotaList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"\n# Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","metadata":{}},{"cell_type":"code","source":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\ntest = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# Preview the data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:24.071291Z","iopub.execute_input":"2021-08-19T16:44:24.071624Z","iopub.status.idle":"2021-08-19T16:44:27.319165Z","shell.execute_reply.started":"2021-08-19T16:44:24.071588Z","shell.execute_reply":"2021-08-19T16:44:27.318235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T15:45:15.107102Z","iopub.execute_input":"2021-08-19T15:45:15.107646Z","iopub.status.idle":"2021-08-19T15:45:15.377305Z","shell.execute_reply.started":"2021-08-19T15:45:15.107604Z","shell.execute_reply":"2021-08-19T15:45:15.376535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the structure of the data\nprint(train.info())","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:27.32081Z","iopub.execute_input":"2021-08-19T16:44:27.321162Z","iopub.status.idle":"2021-08-19T16:44:27.59676Z","shell.execute_reply.started":"2021-08-19T16:44:27.321121Z","shell.execute_reply":"2021-08-19T16:44:27.595649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert Dtypes : ","metadata":{}},{"cell_type":"code","source":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:33.755373Z","iopub.execute_input":"2021-08-19T16:44:33.755718Z","iopub.status.idle":"2021-08-19T16:44:33.845546Z","shell.execute_reply.started":"2021-08-19T16:44:33.755686Z","shell.execute_reply":"2021-08-19T16:44:33.844626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:31.851122Z","iopub.execute_input":"2021-08-19T16:44:31.85151Z","iopub.status.idle":"2021-08-19T16:44:31.8875Z","shell.execute_reply.started":"2021-08-19T16:44:31.851472Z","shell.execute_reply":"2021-08-19T16:44:31.886421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the model features and target\n## Extract X and y ","metadata":{}},{"cell_type":"code","source":"# Create arrays for the features and the response variable\ny = train['target'].to_numpy()\nX = train.drop(['id','target'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T11:22:02.782742Z","iopub.execute_input":"2021-08-20T11:22:02.783222Z","iopub.status.idle":"2021-08-20T11:22:02.803121Z","shell.execute_reply.started":"2021-08-20T11:22:02.783179Z","shell.execute_reply":"2021-08-20T11:22:02.801689Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8fd887f30f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create arrays for the features and the response variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"],"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Create test and train groups\n\nNow we’ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We’ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","metadata":{}},{"cell_type":"code","source":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:38.986172Z","iopub.execute_input":"2021-08-19T16:44:38.986845Z","iopub.status.idle":"2021-08-19T16:44:39.103605Z","shell.execute_reply.started":"2021-08-19T16:44:38.986788Z","shell.execute_reply":"2021-08-19T16:44:39.099655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What should we do for each colmun\n## Separate features by dtype\n\nNext we’ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I’ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don’t want to include the target income column, so I’ve dropped that.\n## Cat Features ","metadata":{}},{"cell_type":"code","source":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:51.681717Z","iopub.execute_input":"2021-08-19T16:44:51.682079Z","iopub.status.idle":"2021-08-19T16:44:51.708579Z","shell.execute_reply.started":"2021-08-19T16:44:51.682045Z","shell.execute_reply":"2021-08-19T16:44:51.707409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Num Features ","metadata":{}},{"cell_type":"code","source":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:53.167371Z","iopub.execute_input":"2021-08-19T16:44:53.167709Z","iopub.status.idle":"2021-08-19T16:44:53.204345Z","shell.execute_reply.started":"2021-08-19T16:44:53.167677Z","shell.execute_reply":"2021-08-19T16:44:53.203404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cat_columns)\nprint(num_columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:55.329808Z","iopub.execute_input":"2021-08-19T16:44:55.330169Z","iopub.status.idle":"2021-08-19T16:44:55.336308Z","shell.execute_reply.started":"2021-08-19T16:44:55.330134Z","shell.execute_reply":"2021-08-19T16:44:55.334778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check that we have all column","metadata":{}},{"cell_type":"code","source":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(all_columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:44:57.754096Z","iopub.execute_input":"2021-08-19T16:44:57.754502Z","iopub.status.idle":"2021-08-19T16:44:57.762692Z","shell.execute_reply.started":"2021-08-19T16:44:57.754468Z","shell.execute_reply":"2021-08-19T16:44:57.761674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:01:37.20199Z","iopub.execute_input":"2021-08-19T12:01:37.202523Z","iopub.status.idle":"2021-08-19T12:01:37.224007Z","shell.execute_reply.started":"2021-08-19T12:01:37.202488Z","shell.execute_reply":"2021-08-19T12:01:37.223003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Cat that occur rarely \n## What is high cardinality?\n\nAlmost all datasets now have categorical variables. Each categorical variable consists of unique values. A categorical feature is said to possess high cardinality when there are too many of these unique values. One-Hot Encoding becomes a big problem in such a case since we have a separate column for each unique value (indicating its presence or absence) in the categorical variable. This leads to two problems, one is obviously space consumption, but this is not as big a problem as the second problem, the curse of dimensionality.\n## The Curse of Dimensionality\n\nHere is a simple summarization:\n\n    As the number of features grows, the amount of data we need to accurately be able to distinguish between these features (in order to give us a prediction) and generalize our model (learned function) grows EXPONENTIALLY.   \nwould like to use Yoshua Bengio’s (Yes the legendary Yoshua Bengio !) quora answer to explain this in more detail. I strongly advise reading the whole answer here. According to the answer, increasing the number of different values in a feature simply increases the total number of possible combinations that can be made using the input row (containing n such features). Say we have two features with two distinct values each, this gives us a total of 4 possible ways to combine the two features. Now if one of these had three distinct values we would have 3X2 =6 possible ways to combine them.\n\nIn classical non-parametric learning algorithms (e.g. nearest-neighbor, Gaussian kernel SVM, Gaussian kernel Gaussian Process, etc.) the model needs to see at least one example for each of these combinations (or at least as many as necessary to cover all the variations of configurations of interest), in order to produce a correct answer, one that is different from the target value required for other nearby configurations.\n\nThere is a workaround to this, that is the model even in the absence of a lot of training data can discern between configurations (not in the training set) for future predictions provided there is some sort of structure (pattern) in these combinations. In most cases, high cardinality makes it difficult for the model to identify such patterns and hence the model doesn’t generalise well to examples outside the training set.    \n## Reducing Cardinality by using a simple Aggregating function\n\nBelow is a simple function I use to reduce the cardinality of a feature. The idea is very simple. Leave instances belonging to a value with high frequency as they are and replace the other instances with a new category which we will call other.\n\n    Choose a threshold\n    Sort unique values in the column by their frequency in descending order\n    Keep adding the frequency of these sorted (descending) unique values until a threshold is reached.\n    These are the unique categories we will keep and instances of all other categories shall be replaced by “other”.\nLet’s run through a quick example before going through the code. Say our column colour has 100 values and our threshold is 90% (that is 90). We have 5 different categories of colours: Red (50), Blue(40), Yellow (5), Green (3) and Orange (2). The numbers within the bracket indicate how many instances of that category are present in the column.\n\nWe see that Red (50)+Blue (40) reaches our threshold of 90. In that case, we retain only 2 categories (Red, Blue) and mark all other instances of other colours as “Other”\nThus we have reduced cardinality from 5 to 3 (Red, Blue, Other)\n\nHere is the utility function I wrote to facilitate this. It’s well commented and follows exactly what I described above so you won’t have a problem following along. We can set a custom threshold and the return_categories option optionally lets us see the list of all unique values after reducing cardinality.\n    ","metadata":{}},{"cell_type":"code","source":"# Create a series out of the Country column\ncat6 = train.cat6\n\n# Get the counts of each category\ncat6_counts = cat6.value_counts()\n\n# Print the count values for each category\nprint(cat6_counts)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T15:46:00.146598Z","iopub.execute_input":"2021-08-19T15:46:00.146931Z","iopub.status.idle":"2021-08-19T15:46:00.21193Z","shell.execute_reply.started":"2021-08-19T15:46:00.146903Z","shell.execute_reply":"2021-08-19T15:46:00.2109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nsns.barplot(cat6_counts.index, cat6_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of countries')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Country', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:08.957261Z","iopub.execute_input":"2021-08-19T12:02:08.957633Z","iopub.status.idle":"2021-08-19T12:02:09.199219Z","shell.execute_reply.started":"2021-08-19T12:02:08.957597Z","shell.execute_reply":"2021-08-19T12:02:09.198235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train['cat7'].astype('category').cat.categories.tolist()\ncounts = train['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:19.954555Z","iopub.execute_input":"2021-08-19T12:02:19.954918Z","iopub.status.idle":"2021-08-19T12:02:20.16516Z","shell.execute_reply.started":"2021-08-19T12:02:19.954872Z","shell.execute_reply":"2021-08-19T12:02:20.163955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_columns :\n    cat_frenquency = (train[col].value_counts())/train.shape[0]\n    botton_decile = cat_frenquency.quantile(q=0.05)\n    print(col , cat_frenquency,botton_decile )   ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:23.524502Z","iopub.execute_input":"2021-08-19T12:02:23.524851Z","iopub.status.idle":"2021-08-19T12:02:23.575952Z","shell.execute_reply.started":"2021-08-19T12:02:23.524816Z","shell.execute_reply":"2021-08-19T12:02:23.574967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Condense Data with Threshold of 0.05","metadata":{}},{"cell_type":"code","source":"def condense_category(col, min_freq=0.05, new_name='other'):\n    series = pd.value_counts(col)\n    mask = (series/series.sum()).lt(min_freq)\n    return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\ntrain_condense=train.copy()\ntrain_condense[cat_columns]=train_condense[cat_columns].apply(condense_category, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:30.984228Z","iopub.execute_input":"2021-08-19T12:02:30.984606Z","iopub.status.idle":"2021-08-19T12:02:31.186591Z","shell.execute_reply.started":"2021-08-19T12:02:30.984572Z","shell.execute_reply":"2021-08-19T12:02:31.185578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_condense.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:34.58614Z","iopub.execute_input":"2021-08-19T12:02:34.58652Z","iopub.status.idle":"2021-08-19T12:02:34.904805Z","shell.execute_reply.started":"2021-08-19T12:02:34.586489Z","shell.execute_reply":"2021-08-19T12:02:34.903924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Convert Ddtype","metadata":{}},{"cell_type":"code","source":"train_condense[train_condense.select_dtypes(['float64']).columns] = train_condense[train_condense.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain_condense[train_condense.select_dtypes(['object']).columns] = train_condense.select_dtypes(['object']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:38.587046Z","iopub.execute_input":"2021-08-19T12:02:38.587403Z","iopub.status.idle":"2021-08-19T12:02:39.071317Z","shell.execute_reply.started":"2021-08-19T12:02:38.587372Z","shell.execute_reply":"2021-08-19T12:02:39.070444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_columns :\n    cat_frenquency = (train_condense[col].value_counts())\n    botton_decile = cat_frenquency.quantile(q=0.05)\n    print(col , cat_frenquency,botton_decile )   ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:40.516289Z","iopub.execute_input":"2021-08-19T12:02:40.516651Z","iopub.status.idle":"2021-08-19T12:02:40.560576Z","shell.execute_reply.started":"2021-08-19T12:02:40.516623Z","shell.execute_reply":"2021-08-19T12:02:40.559643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a series out of the Country column\ncat6 = train_condense.cat6\n\n# Get the counts of each category\ncat6_counts = cat6.value_counts()\n\n# Print the count values for each category\nprint(cat6_counts)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:44.182359Z","iopub.execute_input":"2021-08-19T12:02:44.182726Z","iopub.status.idle":"2021-08-19T12:02:44.191394Z","shell.execute_reply.started":"2021-08-19T12:02:44.182685Z","shell.execute_reply":"2021-08-19T12:02:44.19019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_condense['cat7'].astype('category').cat.categories.tolist()\ncounts = train_condense['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T10:52:36.401733Z","iopub.execute_input":"2021-08-19T10:52:36.402002Z","iopub.status.idle":"2021-08-19T10:52:36.517845Z","shell.execute_reply.started":"2021-08-19T10:52:36.401977Z","shell.execute_reply":"2021-08-19T10:52:36.516861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  X and Y for data Condense ","metadata":{}},{"cell_type":"code","source":"# Create arrays for the features and the response variable\ny_condense = train_condense['target'].to_numpy()\nX_condense = train_condense.drop(['id','target'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train_condense , X_test_condense , y_train_condense , y_test_condense  = train_test_split(X_condense , y_condense , test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test_condense.shape[0], X_train_condense.shape[0], X_test_condense.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:45:10.501521Z","iopub.execute_input":"2021-08-19T16:45:10.501883Z","iopub.status.idle":"2021-08-19T16:45:10.532998Z","shell.execute_reply.started":"2021-08-19T16:45:10.50185Z","shell.execute_reply":"2021-08-19T16:45:10.531225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Condense Data with Threshold of 0.1","metadata":{}},{"cell_type":"code","source":"def condense_category(col, min_freq=0.1, new_name='other'):\n    series = pd.value_counts(col)\n    mask = (series/series.sum()).lt(min_freq)\n    return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\ntrain_condense2=train.copy()\ntrain_condense2[cat_columns]=train_condense2[cat_columns].apply(condense_category, axis=0)\ntrain_condense2[train_condense2.select_dtypes(['float64']).columns] = train_condense2[train_condense2.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain_condense[train_condense2.select_dtypes(['object']).columns] = train_condense2.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n# Create arrays for the features and the response variable\ny_condense2 = train_condense2['target'].to_numpy()\nX_condense2 = train_condense2.drop(['id','target'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train_condense2 , X_test_condense2 , y_train_condense2 , y_test_condense2  = train_test_split(X_condense2 , y_condense2 , test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test_condense2.shape[0], X_train_condense2.shape[0], X_test_condense2.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:02:52.509025Z","iopub.execute_input":"2021-08-19T12:02:52.50956Z","iopub.status.idle":"2021-08-19T12:02:53.248801Z","shell.execute_reply.started":"2021-08-19T12:02:52.509515Z","shell.execute_reply":"2021-08-19T12:02:53.247785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create complexe transformer  in order to  put all transformations in the same pipe \n    'num_columns' :Cleaning->Valeur Manquante -> Standar_Scaler\n    'cat_columns' : Cleaning -> Valeur Manquante -> Categorique [One Hot]\n\n\n    fill_missing_then_Standar_scaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n        StandardScaler()\n    )\n\n    \"Write a pattern to extract numbers and decimals\"\n    def return_number(string):\n        pattern = re.compile(r\"\\d+\\.\\d+\")\n        # Search the text for matches\n        number = re.match(pattern, string)\n        # If a value is returned, use group(0) to return the found value\n        if number is not None:\n            return float(number.group(0))\n\n    extraire_number_then_imput_then_scale = make_pipeline(\n        FunctionTransformer(extract_number),\n        fill_missing_then_Standar_scaler,\n    )    \n","metadata":{}},{"cell_type":"markdown","source":"# Pipe Cat ","metadata":{}},{"cell_type":"code","source":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_encoder1 = make_pipeline(\n    SimpleImputer(strategy='most_frequent', add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_one_hot_LabelEncoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    LabelEncoder()\n)\nfill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder()\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:45:35.460355Z","iopub.execute_input":"2021-08-19T16:45:35.46071Z","iopub.status.idle":"2021-08-19T16:45:35.467955Z","shell.execute_reply.started":"2021-08-19T16:45:35.460679Z","shell.execute_reply":"2021-08-19T16:45:35.466689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipe Num  :","metadata":{}},{"cell_type":"code","source":"fill_missing_then_StandardScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\nfill_missing_then_StandardScalerPolynomialFeatures = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n     PolynomialFeatures(degree=2),StandardScaler()\n)\nfill_missing_then_RobustScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    RobustScaler()\n)\nfill_missing_then_MinMaxScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    MinMaxScaler()\n)\nfill_missing_then_Outlier_MinMax = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    MinMaxScaler()\n) ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:45:38.557686Z","iopub.execute_input":"2021-08-19T16:45:38.558036Z","iopub.status.idle":"2021-08-19T16:45:38.566361Z","shell.execute_reply.started":"2021-08-19T16:45:38.558002Z","shell.execute_reply":"2021-08-19T16:45:38.565547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compose num+cat : ColumnTransformer\n## First Standar preprocess","metadata":{}},{"cell_type":"code","source":"data_preprocessOrdinalEncoder = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns)\n)\ndata_preprocessone_hot_encoder = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns)\n)\ndata_preprocessoneOnehotPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:45:40.916632Z","iopub.execute_input":"2021-08-19T16:45:40.916975Z","iopub.status.idle":"2021-08-19T16:45:40.922238Z","shell.execute_reply.started":"2021-08-19T16:45:40.916942Z","shell.execute_reply":"2021-08-19T16:45:40.921069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check\n","metadata":{}},{"cell_type":"code","source":"data_preprocessone_hot_encoder.fit(X_train)\ndata_preprocessone_hot_encoder.transform(X_train)\ndata_preprocessone_hot_encoder.transform(X_test)\nprint(\"Ok , Every thing is well \")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:45:44.651414Z","iopub.execute_input":"2021-08-19T16:45:44.651779Z","iopub.status.idle":"2021-08-19T16:45:48.701057Z","shell.execute_reply.started":"2021-08-19T16:45:44.651749Z","shell.execute_reply":"2021-08-19T16:45:48.700095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_preprocessone_hot_encoder.fit(X_train_condense)\ndata_preprocessone_hot_encoder.transform(X_train_condense)\ndata_preprocessone_hot_encoder.transform(X_test_condense)\nprint(\"Ok , Every thing is well \")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T11:45:10.522239Z","iopub.execute_input":"2021-08-20T11:45:10.522616Z","iopub.status.idle":"2021-08-20T11:45:10.545583Z","shell.execute_reply.started":"2021-08-20T11:45:10.522585Z","shell.execute_reply":"2021-08-20T11:45:10.544089Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-75b0e6729104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_preprocessone_hot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_condense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_preprocessone_hot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_condense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_preprocessone_hot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_condense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ok , Every thing is well \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_preprocessone_hot_encoder' is not defined"],"ename":"NameError","evalue":"name 'data_preprocessone_hot_encoder' is not defined","output_type":"error"}]},{"cell_type":"code","source":"data_preprocessone_hot_encoder.fit(X_train_condense2)\ndata_preprocessone_hot_encoder.transform(X_train_condense2)\ndata_preprocessone_hot_encoder.transform(X_test_condense2)\nprint(\"Ok , Every thing is well \")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:14.617823Z","iopub.execute_input":"2021-08-19T12:03:14.618115Z","iopub.status.idle":"2021-08-19T12:03:18.942761Z","shell.execute_reply.started":"2021-08-19T12:03:14.618087Z","shell.execute_reply":"2021-08-19T12:03:18.941709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bin Target :","metadata":{}},{"cell_type":"code","source":"train.target.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:18.944075Z","iopub.execute_input":"2021-08-19T12:03:18.944373Z","iopub.status.idle":"2021-08-19T12:03:18.965516Z","shell.execute_reply.started":"2021-08-19T12:03:18.944345Z","shell.execute_reply":"2021-08-19T12:03:18.964791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins = [0.140329,7.742071,  8.728634, 10.411992]\n# Bin labels\nlabels1 = [ 'Low', 'Medium', 'High']\ntrainessai=train.copy()\n# Bin the continuous variable ConvertedSalary using these boundaries\ntrainessai['target_binned'] = pd.cut(trainessai['target'], \n                                         bins=bins,labels=labels1 )","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:18.966561Z","iopub.execute_input":"2021-08-19T12:03:18.966817Z","iopub.status.idle":"2021-08-19T12:03:18.99529Z","shell.execute_reply.started":"2021-08-19T12:03:18.966792Z","shell.execute_reply":"2021-08-19T12:03:18.994372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = trainessai['target_binned'].astype('category').cat.categories.tolist()\ncounts = trainessai['target_binned'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:18.996831Z","iopub.execute_input":"2021-08-19T12:03:18.997134Z","iopub.status.idle":"2021-08-19T12:03:19.116677Z","shell.execute_reply.started":"2021-08-19T12:03:18.997087Z","shell.execute_reply":"2021-08-19T12:03:19.115454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# t-SNE visualization of high-dimensional data¶\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset. press","metadata":{}},{"cell_type":"code","source":"m = TSNE(learning_rate=50)\ndf_numeric =trainessai.drop(['id','target'], axis=1).iloc[:5000,:]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(df_numeric)\n%timeit \nprint(tsne_features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T10:52:50.614163Z","iopub.execute_input":"2021-08-19T10:52:50.614521Z","iopub.status.idle":"2021-08-19T10:59:40.468013Z","shell.execute_reply.started":"2021-08-19T10:52:50.614493Z","shell.execute_reply":"2021-08-19T10:59:40.467202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataa=trainessai.drop(['id','target'], axis=1).iloc[:5000,:]\ndataa['x']=tsne_features[:, 0]\ndataa['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='target_binned', data=dataa)\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T11:05:06.229566Z","iopub.execute_input":"2021-08-19T11:05:06.230131Z","iopub.status.idle":"2021-08-19T11:05:09.55396Z","shell.execute_reply.started":"2021-08-19T11:05:06.23007Z","shell.execute_reply":"2021-08-19T11:05:09.552881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The curse of dimensionality\n\nIn fact, to avoid overfitting the number of observations should increase exponentially with the number of features. Since this becomes really problematic for high dimensional datasets this phenomenon is known as the curse of dimensionality. The solution to this is of course to apply dimensionality reduction.\n\n# Features with missing values or little variance","metadata":{}},{"cell_type":"code","source":"# Create the boxplot\ntrainessai.boxplot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:20.675493Z","iopub.execute_input":"2021-08-19T12:03:20.675847Z","iopub.status.idle":"2021-08-19T12:03:29.3699Z","shell.execute_reply.started":"2021-08-19T12:03:20.675796Z","shell.execute_reply":"2021-08-19T12:03:29.36901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\ndf_numeric =trainessai.drop(['id','target'], axis=1)._get_numeric_data()\nnormalized_df = df_numeric/df_numeric.mean()\n\nnormalized_df.boxplot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:29.371285Z","iopub.execute_input":"2021-08-19T12:03:29.371572Z","iopub.status.idle":"2021-08-19T12:03:37.101831Z","shell.execute_reply.started":"2021-08-19T12:03:29.371543Z","shell.execute_reply":"2021-08-19T12:03:37.100599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainessai.var()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:37.103704Z","iopub.execute_input":"2021-08-19T12:03:37.104109Z","iopub.status.idle":"2021-08-19T12:03:37.131261Z","shell.execute_reply.started":"2021-08-19T12:03:37.104078Z","shell.execute_reply":"2021-08-19T12:03:37.130311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\nnormalized_df = df_numeric / df_numeric.mean()\n\n# Print the variances of the normalized data\nprint(normalized_df.var())","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:37.132638Z","iopub.execute_input":"2021-08-19T12:03:37.132951Z","iopub.status.idle":"2021-08-19T12:03:37.175368Z","shell.execute_reply.started":"2021-08-19T12:03:37.132916Z","shell.execute_reply":"2021-08-19T12:03:37.174342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Finding a good variance threshold\n\nQuestion Inspect the printed variances. If you want to remove the 2 very low variance features. What would be a good variance threshold? Possible Answers\n ==> A threshold of  (0.19) will remove the two low variance features.\n## Features with low variance\n\nIn the previous exercise you established that 0.19 is a good threshold to filter out low variance features in head_df after normalization. Now use the VarianceThreshold feature selector to remove these features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\n# Create a VarianceThreshold feature selector\nsel = VarianceThreshold(threshold=0.19)\n\n# Fit the selector to normalized head_df\nsel.fit(df_numeric / df_numeric.mean())\n\n# Create a boolean mask\nmask = sel.get_support()\n\n# Apply the mask to create a reduced dataframe\nreduced_df = df_numeric.loc[:, mask]\n\nprint(\"Dimensionality reduced from {} to {}.\".format(df_numeric.shape[1], reduced_df.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:37.249389Z","iopub.execute_input":"2021-08-19T12:03:37.249748Z","iopub.status.idle":"2021-08-19T12:03:37.337155Z","shell.execute_reply.started":"2021-08-19T12:03:37.249713Z","shell.execute_reply":"2021-08-19T12:03:37.33611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"cont7 can be  removed ","metadata":{}},{"cell_type":"markdown","source":"# Removing redundant features\n## Selecting relevant features\nVisually detecting redundant features","metadata":{}},{"cell_type":"markdown","source":"### Pairwise correlation","metadata":{}},{"cell_type":"code","source":"trainessai.corr()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T11:12:26.157568Z","iopub.execute_input":"2021-08-19T11:12:26.157941Z","iopub.status.idle":"2021-08-19T11:12:26.417964Z","shell.execute_reply.started":"2021-08-19T11:12:26.15791Z","shell.execute_reply":"2021-08-19T11:12:26.416763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns \n# Create the correlation matrix\ncorr = trainessai.drop(['id'], axis=1).corr()\ncmap = sns.diverging_palette(h_neg=10,  h_pos=240, as_cmap=True)\n# Draw the heatmap\nsns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:12:00.505112Z","iopub.execute_input":"2021-08-19T12:12:00.505797Z","iopub.status.idle":"2021-08-19T12:12:02.382505Z","shell.execute_reply.started":"2021-08-19T12:12:00.505746Z","shell.execute_reply":"2021-08-19T12:12:02.381038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a mask for the upper triangle \nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Add the mask to the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:12:06.170462Z","iopub.execute_input":"2021-08-19T12:12:06.170837Z","iopub.status.idle":"2021-08-19T12:12:07.036389Z","shell.execute_reply.started":"2021-08-19T12:12:06.170803Z","shell.execute_reply":"2021-08-19T12:12:07.03548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing highly correlated features","metadata":{}},{"cell_type":"code","source":"# Calculate the correlation matrix and take the absolute value\ncorr_matrix = trainessai.drop(['id'], axis=1).corr().abs()\n\n# Create a True/False mask and apply it\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ntri_df = corr_matrix.mask(mask)\n\n# List column names of highly correlated features (r > 0.95)\nto_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.4)]\n\n# Drop the features in the to_drop list\nreduced_df = trainessai.drop(['id'], axis=1).drop(to_drop, axis=1)\nprint(to_drop)\nprint(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:03:47.668903Z","iopub.execute_input":"2021-08-19T12:03:47.66927Z","iopub.status.idle":"2021-08-19T12:03:47.923998Z","shell.execute_reply.started":"2021-08-19T12:03:47.669242Z","shell.execute_reply":"2021-08-19T12:03:47.92297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"['cont0', 'cont5', 'cont8', 'cont9', 'cont10']\nThe reduced dataframe has 21 columns.","metadata":{}},{"cell_type":"markdown","source":"#  Automatic Recursive Feature Elimination\n","metadata":{}},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:23:24.987009Z","iopub.execute_input":"2021-08-19T12:23:24.987405Z","iopub.status.idle":"2021-08-19T12:23:24.994081Z","shell.execute_reply.started":"2021-08-19T12:23:24.987369Z","shell.execute_reply":"2021-08-19T12:23:24.993057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mask1 :","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE with a LinearRegression estimator and 20 features to select\nrfe_LR = RFE(estimator=LinearRegression(), n_features_to_select = 20, verbose=1)\n\n# Fits the eliminator to the data\nrfe_LR.fit(data_preprocessOrdinalEncoder.fit_transform(X_train), y_train)\n\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe_LR.ranking_)))\n\n# Print the features that are not eliminated\nprint(X.columns[rfe_LR.support_])\n\n# Calculates the test set accuracy\nrmse = mean_squared_error(y_test, rfe_LR.predict(data_preprocessOrdinalEncoder.transform(X_test)), squared=False)\nprint(\"{} rmse on test set.\".format(rmse))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:57:44.162204Z","iopub.execute_input":"2021-08-19T12:57:44.162837Z","iopub.status.idle":"2021-08-19T12:58:05.813919Z","shell.execute_reply.started":"2021-08-19T12:57:44.1628Z","shell.execute_reply":"2021-08-19T12:58:05.812645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_LR=rfe_LR.support_\nmask_LR","metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:58:48.343937Z","iopub.execute_input":"2021-08-19T12:58:48.344311Z","iopub.status.idle":"2021-08-19T12:58:48.349904Z","shell.execute_reply.started":"2021-08-19T12:58:48.34428Z","shell.execute_reply":"2021-08-19T12:58:48.349148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Index(['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cont0',\n       'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9',\n       'cont10', 'cont11', 'cont12', 'cont13'],\n      dtype='object')\n      \n    0.7418047219576385 rmse on test set.\n    array([ True,  True,  True,  True,  True,  True,  True, False, False,\n       False,  True, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True])","metadata":{}},{"cell_type":"markdown","source":"## Mask 2","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE with a LinearRegression estimator and 20 features to select\nrfe_RFR  = RFE(estimator=RandomForestRegressor(n_jobs=-1), n_features_to_select = 20, verbose=1)\n# Fits the eliminator to the data\nrfe_RFR.fit(data_preprocessOrdinalEncoder.fit_transform(X_train), y_train)\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe_RFR.ranking_)))\n# Print the features that are not eliminated\nprint(X.columns[rfe_RFR.support_])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:24:53.666394Z","iopub.execute_input":"2021-08-19T13:24:53.666762Z","iopub.status.idle":"2021-08-19T13:49:08.654988Z","shell.execute_reply.started":"2021-08-19T13:24:53.66673Z","shell.execute_reply":"2021-08-19T13:49:08.652741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculates the test set accuracy\nrmse = mean_squared_error(y_test, rfe_RFR.predict(data_preprocessOrdinalEncoder.transform(X_test)), squared=False)\nprint(\"{} rmse on test set.\".format(rmse))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:03:14.74551Z","iopub.execute_input":"2021-08-19T14:03:14.745897Z","iopub.status.idle":"2021-08-19T14:03:15.926162Z","shell.execute_reply.started":"2021-08-19T14:03:14.745851Z","shell.execute_reply":"2021-08-19T14:03:15.925214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_RFR=rfe_RFR.support_\nmask_RFR","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:03:32.378238Z","iopub.execute_input":"2021-08-19T14:03:32.378612Z","iopub.status.idle":"2021-08-19T14:03:32.384845Z","shell.execute_reply.started":"2021-08-19T14:03:32.378578Z","shell.execute_reply":"2021-08-19T14:03:32.383883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    0.73964538954687 rmse on test set.\n    array([ True, False, False,  True, False,  True, False,  True,  True,\n            True,  True,  True,  True,  True,  True,  True,  True,  True,\n            True,  True,  True,  True,  True,  True])","metadata":{}},{"cell_type":"markdown","source":"## Mask3","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE with a LinearRegression estimator and 20 features to select\nrfe_GBR = RFE(estimator=GradientBoostingRegressor(), n_features_to_select = 20, verbose=1)\n\n# Fits the eliminator to the data\nrfe_GBR.fit(data_preprocessOrdinalEncoder.fit_transform(X_train), y_train)\n\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe_GBR.ranking_)))\n\n# Print the features that are not eliminated\nprint(X.columns[rfe_GBR.support_])\n\n# Calculates the test set accuracy\nrmse = mean_squared_error(y_test, rfe_GBR.predict(data_preprocessOrdinalEncoder.transform(X_test)), squared=False)\nprint(\"{} rmse on test set.\".format(rmse))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:23:18.878785Z","iopub.execute_input":"2021-08-19T13:23:18.87922Z","iopub.status.idle":"2021-08-19T13:23:24.59478Z","shell.execute_reply.started":"2021-08-19T13:23:18.879184Z","shell.execute_reply":"2021-08-19T13:23:24.59197Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_GBR=rfe_GBR.support_\nmask_GBR","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:21:37.096205Z","iopub.execute_input":"2021-08-19T13:21:37.096583Z","iopub.status.idle":"2021-08-19T13:21:37.102793Z","shell.execute_reply.started":"2021-08-19T13:21:37.096551Z","shell.execute_reply":"2021-08-19T13:21:37.101862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Index(['cat1', 'cat3', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1',\n           'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9',\n           'cont10', 'cont11', 'cont12', 'cont13'],\n          dtype='object')\n    0.7337589093625022 rmse on test set.\n    array([False,  True, False,  True, False, False,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True])","metadata":{}},{"cell_type":"markdown","source":"## Mask 4","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE with a LinearRegression estimator and 20 features to select\nrfe_Ridge = RFE(estimator= Ridge(alpha=0.01), n_features_to_select = 20, verbose=1)\n# Fits the eliminator to the data\nrfe_Ridge.fit(data_preprocessOrdinalEncoder.fit_transform(X_train), y_train)\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X_train.columns, rfe_Ridge.ranking_)))\n# Print the features that are not eliminated\nprint(X.columns[rfe_Ridge.support_])\n# Calculates the test set accuracy\nrmse = mean_squared_error(y_test, rfe_Ridge.predict(data_preprocessOrdinalEncoder.transform(X_test)), squared=False)\nprint(\"{} rmse on test set.\".format(rmse))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:13:40.138509Z","iopub.execute_input":"2021-08-19T14:13:40.138933Z","iopub.status.idle":"2021-08-19T14:14:01.883536Z","shell.execute_reply.started":"2021-08-19T14:13:40.138881Z","shell.execute_reply":"2021-08-19T14:14:01.882468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_Ridge=rfe_Ridge.support_\nmask_Ridge","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:14:08.415009Z","iopub.execute_input":"2021-08-19T14:14:08.4154Z","iopub.status.idle":"2021-08-19T14:14:08.420601Z","shell.execute_reply.started":"2021-08-19T14:14:08.415363Z","shell.execute_reply":"2021-08-19T14:14:08.419937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    0.7418047219647409 rmse on test set.\n    array([ True,  True,  True,  True,  True,  True,  True, False, False,\n           False,  True, False,  True,  True,  True,  True,  True,  True,\n            True,  True,  True,  True,  True,  True])","metadata":{}},{"cell_type":"markdown","source":"## Mask5 ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE with a LinearRegression estimator and 20 features to select\nrfe_Lasso = RFE(estimator= Lasso(), n_features_to_select = 20, verbose=1)\n# Fits the eliminator to the data\nrfe_Lasso.fit(data_preprocessOrdinalEncoder.fit_transform(X_train), y_train)\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X_train.columns, rfe_Lasso.ranking_)))\n# Print the features that are not eliminated\nprint(X.columns[rfe_Lasso.support_])\n# Calculates the test set accuracy\nrmse = mean_squared_error(y_test, rfe_Lasso.predict(data_preprocessOrdinalEncoder.transform(X_test)), squared=False)\nprint(\"{} rmse on test set.\".format(rmse))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:17:22.520642Z","iopub.execute_input":"2021-08-19T14:17:22.521005Z","iopub.status.idle":"2021-08-19T14:17:44.160141Z","shell.execute_reply.started":"2021-08-19T14:17:22.520974Z","shell.execute_reply":"2021-08-19T14:17:44.158839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_Lasso=rfe_Lasso.support_\nmask_Lasso","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:18:08.421949Z","iopub.execute_input":"2021-08-19T14:18:08.422312Z","iopub.status.idle":"2021-08-19T14:18:08.427742Z","shell.execute_reply.started":"2021-08-19T14:18:08.422281Z","shell.execute_reply":"2021-08-19T14:18:08.427027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask6","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE with a LinearRegression estimator and 20 features to select\nrfe_ElasticNet= RFE(estimator= ElasticNet(alpha = 0.01), n_features_to_select = 20, verbose=1)\n# Fits the eliminator to the data\nrfe_ElasticNet.fit(data_preprocessOrdinalEncoder.fit_transform(X_train), y_train)\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X_train.columns, rfe_ElasticNet.ranking_)))\n# Print the features that are not eliminated\nprint(X.columns[rfe_ElasticNet.support_])\n# Calculates the test set accuracy\nrmse = mean_squared_error(y_test, rfe_ElasticNet.predict(data_preprocessOrdinalEncoder.transform(X_test)), squared=False)\nprint(\"{} rmse on test set.\".format(rmse))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:26:49.318982Z","iopub.execute_input":"2021-08-19T14:26:49.319388Z","iopub.status.idle":"2021-08-19T14:27:11.513163Z","shell.execute_reply.started":"2021-08-19T14:26:49.319354Z","shell.execute_reply":"2021-08-19T14:27:11.512021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_ElasticNet=rfe_ElasticNet.support_\nmask_ElasticNet","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:27:31.935221Z","iopub.execute_input":"2021-08-19T14:27:31.935573Z","iopub.status.idle":"2021-08-19T14:27:31.942175Z","shell.execute_reply.started":"2021-08-19T14:27:31.935541Z","shell.execute_reply":"2021-08-19T14:27:31.941138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    0.7424049471095415 rmse on test set.\n    array([False,  True, False,  True, False,  True,  True,  True,  True,\n            True,  True, False,  True,  True,  True,  True,  True,  True,\n            True,  True,  True,  True,  True,  True])","metadata":{}},{"cell_type":"markdown","source":"# Combining features selector","metadata":{}},{"cell_type":"code","source":"# Sum the votes of the three models\nvotes = np.sum([mask_LR, mask_RFR, mask_GBR,mask_SVR,\n                mask_Ridge,mask_Lasso,mask_ElasticNet], axis=0)\n\n# Create a mask for features selected by all 5 models\nmeta_mask = votes >= 4\n\n# Apply the dimensionality reduction on X\nX_reduced = X.loc[:, meta_mask]\nprint(X.columns)\nprint(X_reduced.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:41:04.257742Z","iopub.execute_input":"2021-08-19T14:41:04.258159Z","iopub.status.idle":"2021-08-19T14:41:04.275083Z","shell.execute_reply.started":"2021-08-19T14:41:04.258121Z","shell.execute_reply":"2021-08-19T14:41:04.273989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp1=set(X.columns)\ns=set(X_reduced.columns)\ntemp3 = [x for x in temp1 if x not in s]\nprint(\"list to drop {}\".format(temp3))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T14:41:04.989462Z","iopub.execute_input":"2021-08-19T14:41:04.989995Z","iopub.status.idle":"2021-08-19T14:41:04.995701Z","shell.execute_reply.started":"2021-08-19T14:41:04.989955Z","shell.execute_reply":"2021-08-19T14:41:04.994955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"list to drop ['cat0', 'cat2', 'cont1', 'cat4'] +","metadata":{}},{"cell_type":"markdown","source":"# Features Engineer : ","metadata":{}},{"cell_type":"markdown","source":"# Pipe with PolynomialFeatures","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:04:24.184409Z","iopub.execute_input":"2021-08-19T16:04:24.18474Z","iopub.status.idle":"2021-08-19T16:04:24.859655Z","shell.execute_reply.started":"2021-08-19T16:04:24.18471Z","shell.execute_reply":"2021-08-19T16:04:24.858748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing libraries for polynomial transform\nfrom xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nXGBR = XGBRegressor(n_estimators = 2000,learning_rate = 0.03,\n                    random_state = 22, tree_method = 'gpu_hist', reg_alpha = 10)\nfrom sklearn.preprocessing import PolynomialFeatures\npipepoly = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n        ('classifier', XGBR)])\n\n# Fit the pipeline to the training data\npipepoly.fit(X_train, y_train)\npreds_valid = pipepoly.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:19:59.59655Z","iopub.execute_input":"2021-08-19T16:19:59.596886Z","iopub.status.idle":"2021-08-19T16:20:32.529515Z","shell.execute_reply.started":"2021-08-19T16:19:59.596849Z","shell.execute_reply":"2021-08-19T16:20:32.52874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#0.725598888349985","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sparse interation ","metadata":{}},{"cell_type":"code","source":"class SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n            \n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n            \n        spi = self._create_sparse_interactions(X)\n        return spi\n    \n    \n    def get_feature_names(self):\n        return self.feature_names\n    \n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n        \n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n                \n                # get column multiplications value\n                out = X[:, col_ixs[0]]    \n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)\n# Outlier Handle \nclass OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal) ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:46:21.713926Z","iopub.execute_input":"2021-08-19T16:46:21.714279Z","iopub.status.idle":"2021-08-19T16:46:21.729262Z","shell.execute_reply.started":"2021-08-19T16:46:21.714246Z","shell.execute_reply":"2021-08-19T16:46:21.72798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing libraries for polynomial transform\nfrom xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nXGBR = XGBRegressor(n_estimators = 2000,learning_rate = 0.03,\n                    random_state = 22, tree_method = 'gpu_hist', reg_alpha = 10)\nfrom sklearn.preprocessing import PolynomialFeatures\npipepoly = Pipeline([\n        ('scaler+ONE+poly', data_preprocessoneOnehotPolynomialFeatures),\n         ('dim_red', SelectKBest(f_regression, k=10)),\n         ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n\n# Fit the pipeline to the training data\npipepoly.fit(X_train, y_train)\npreds_valid = pipepoly.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:56:50.504382Z","iopub.execute_input":"2021-08-19T16:56:50.50593Z","iopub.status.idle":"2021-08-19T16:57:16.476387Z","shell.execute_reply.started":"2021-08-19T16:56:50.505893Z","shell.execute_reply":"2021-08-19T16:57:16.475623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_preprocessOrdinalEncoderPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)\ndata_preprocessoneOnehotPolynomialFeatures = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScalerPolynomialFeatures, num_columns)\n)\n# importing libraries for polynomial transform\nfrom xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nXGBR = XGBRegressor(n_estimators = 2000,learning_rate = 0.03,\n                    random_state = 22, tree_method = 'gpu_hist', reg_alpha = 10)\nfrom sklearn.preprocessing import PolynomialFeatures\npipepolyordi = Pipeline([\n        ('scaler+ONE+poly', data_preprocessOrdinalEncoderPolynomialFeatures),\n        ('classifier', XGBR)])\n\n# Fit the pipeline to the training data\npipepolyordi.fit(X_train, y_train)\npreds_valid = pipepolyordi.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:58:45.095577Z","iopub.execute_input":"2021-08-19T16:58:45.095975Z","iopub.status.idle":"2021-08-19T16:59:28.913298Z","shell.execute_reply.started":"2021-08-19T16:58:45.095938Z","shell.execute_reply":"2021-08-19T16:59:28.912337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.7258725244969303\n","metadata":{}},{"cell_type":"markdown","source":"# Pipe With PCA ","metadata":{}},{"cell_type":"code","source":"XGBR = XGBRegressor(n_estimators = 2000,learning_rate = 0.03,\n                    random_state = 22, tree_method = 'gpu_hist', reg_alpha = 10)\nfrom sklearn.preprocessing import PolynomialFeatures\npipepolypca = Pipeline([\n        ('scaler+ONE+poly', data_preprocessOrdinalEncoderPolynomialFeatures),\n        ('interactions', SparseInteractions(degree=2)),\n        ('reducer', PCA(n_components=0.9)),\n        ('classifier', XGBR)])\n\n# Fit the pipeline to the training data\npipepolypca.fit(X_train, y_train)\npreds_valid= pipepolypca.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:32:25.718966Z","iopub.execute_input":"2021-08-19T16:32:25.719315Z","iopub.status.idle":"2021-08-19T16:32:57.567454Z","shell.execute_reply.started":"2021-08-19T16:32:25.719282Z","shell.execute_reply":"2021-08-19T16:32:57.566743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prints the explained variance ratio and accuracy\nprint(pipepolypca.steps[1][1].explained_variance_ratio_)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T16:31:05.004402Z","iopub.execute_input":"2021-08-19T16:31:05.004735Z","iopub.status.idle":"2021-08-19T16:31:05.013586Z","shell.execute_reply.started":"2021-08-19T16:31:05.004704Z","shell.execute_reply":"2021-08-19T16:31:05.011169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chose best Encoder : \nhttp://www.xavierdupre.fr/app/papierstat/helpsphinx/notebooks/artificiel_category_2.html\nhttps://www.kaggle.com/subinium/11-categorical-encoders-and-benchmark\n\n\nhttps://practicaldatascience.co.uk/machine-learning/how-to-use-category-encoders-to-transform-categorical-variables\nhttps://www.kaggle.com/discdiver/category-encoders-examples\nhttps://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\nhttps://pythonrepo.com/repo/scikit-learn-contrib-category_encoders-python-sklearn-utilities","metadata":{"execution":{"iopub.status.busy":"2021-08-19T10:59:40.996013Z","iopub.status.idle":"2021-08-19T10:59:40.999574Z"}}},{"cell_type":"code","source":"encoders = {\n    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n    'BaseNEncoder': ce.basen.BaseNEncoder,\n    'BinaryEncoder': ce.binary.BinaryEncoder,\n    'CatBoostEncoder': ce.cat_boost.CatBoostEncoder,\n    'HashingEncoder': ce.hashing.HashingEncoder,\n    'HelmertEncoder': ce.helmert.HelmertEncoder,\n    'JamesSteinEncoder': ce.james_stein.JamesSteinEncoder,\n    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n    'LeaveOneOutEncoder': ce.leave_one_out.LeaveOneOutEncoder,\n    'MEstimateEncoder': ce.m_estimate.MEstimateEncoder,\n    'OrdinalEncoder': ce.ordinal.OrdinalEncoder,\n    'PolynomialEncoder': ce.polynomial.PolynomialEncoder,\n    'SumEncoder': ce.sum_coding.SumEncoder,\n    'TargetEncoder': ce.target_encoder.TargetEncoder,\n    'WOEEncoder': ce.woe.WOEEncoder\n}\ndf_resultsXGBR = pd.DataFrame(columns=['encoder', 'r2', 'mse', 'rmse'])\n\nfor key in encoders:\n    try :\n\n        categorical_transformer = Pipeline(\n            steps=[\n                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n                ('encoder', encoders[key]())\n            ]\n        )    \n\n        numeric_transformer = Pipeline(\n            steps=[\n                ('imputer', SimpleImputer(strategy='mean')),\n                ('scaler', StandardScaler())\n            ]\n        )\n\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer, num_columns),\n                ('categorical', categorical_transformer, cat_columns)\n            ]\n        )\n\n        pipe_XGBR = Pipeline(\n            steps=[\n                ('preprocessor', preprocessor),\n                ('classifier', Ridge(alpha=0.01))\n            ]\n        )\n\n        modelpipe_XGBR = pipe_XGBR.fit(X_train, y_train)\n        y_pred = modelpipe_XGBR.predict(X_test)\n\n        row = {\n            'encoder': key,\n            'r2': modelpipe_XGBR.score(X_test, y_test),\n            'mse': mean_squared_error(y_test, y_pred, squared=True),\n            'rmse': mean_squared_error(y_test, y_pred,squared=False)\n        }\n        df_resultsXGBR = df_resultsXGBR.append(row, ignore_index=True)\n    except :\n        raw={\n            'encoder': key,\n            'r2': np.nan,\n            'mse': np.nan,\n            'rmse': np.nan\n        }\n        df_resultsXGBR = df_resultsXGBR.append(row, ignore_index=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T17:45:35.888672Z","iopub.execute_input":"2021-08-19T17:45:35.889061Z","iopub.status.idle":"2021-08-19T17:50:07.754142Z","shell.execute_reply.started":"2021-08-19T17:45:35.889028Z","shell.execute_reply":"2021-08-19T17:50:07.753197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_resultsXGBR.head(20).sort_values(by='rmse')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T17:51:31.564569Z","iopub.execute_input":"2021-08-19T17:51:31.564945Z","iopub.status.idle":"2021-08-19T17:51:31.579369Z","shell.execute_reply.started":"2021-08-19T17:51:31.564913Z","shell.execute_reply":"2021-08-19T17:51:31.578282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lR :\n\n        encoder \tr2 \tmse \trmse\n        1 \tBaseNEncoder \t0.023040 \t0.548627 \t0.740694\n        2 \tBinaryEncoder \t0.023040 \t0.548627 \t0.740694\n        0 \tBackwardDifferenceEncoder \t0.022985 \t0.548658 \t0.740714\n        7 \tOneHotEncoder \t0.022985 \t0.548658 \t0.740714\n        11 \tPolynomialEncoder \t0.022985 \t0.548658 \t0.740714\n        12 \tSumEncoder \t0.022985 \t0.548658 \t0.740714\n        5 \tHelmertEncoder \t0.022985 \t0.548658 \t0.740714\n        9 \tMEstimateEncoder \t0.022730 \t0.548801 \t0.740811\n        13 \tTargetEncoder \t0.022728 \t0.548802 \t0.740812\n        14 \tTargetEncoder \t0.022728 \t0.548802 \t0.740812\n        6 \tJamesSteinEncoder \t0.022727 \t0.548803 \t0.740812\n        8 \tLeaveOneOutEncoder \t0.022723 \t0.548805 \t0.740814\n        3 \tCatBoostEncoder \t0.022414 \t0.548978 \t0.740931\n        10 \tOrdinalEncoder \t0.020120 \t0.550266 \t0.741799\n        4 \tHashingEncoder \t0.018452 \t0.551203 \t0.742431","metadata":{}},{"cell_type":"code","source":"XGBR = XGBRegressor(n_estimators = 2000,learning_rate = 0.03,\n                    random_state = 22, tree_method = 'gpu_hist', reg_alpha = 10)\nencoders = {\n    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n    'BaseNEncoder': ce.basen.BaseNEncoder,\n    'BinaryEncoder': ce.binary.BinaryEncoder,\n    'CatBoostEncoder': ce.cat_boost.CatBoostEncoder,\n    'HashingEncoder': ce.hashing.HashingEncoder,\n    'HelmertEncoder': ce.helmert.HelmertEncoder,\n    'JamesSteinEncoder': ce.james_stein.JamesSteinEncoder,\n    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n    'LeaveOneOutEncoder': ce.leave_one_out.LeaveOneOutEncoder,\n    'MEstimateEncoder': ce.m_estimate.MEstimateEncoder,\n    'OrdinalEncoder': ce.ordinal.OrdinalEncoder,\n    'PolynomialEncoder': ce.polynomial.PolynomialEncoder,\n    'SumEncoder': ce.sum_coding.SumEncoder,\n    'TargetEncoder': ce.target_encoder.TargetEncoder,\n    'WOEEncoder': ce.woe.WOEEncoder\n}\ndf_resultsXGBR = pd.DataFrame(columns=['encoder', 'r2', 'mse', 'rmse'])\n\nfor key in encoders:\n    try :\n\n        categorical_transformer = Pipeline(\n            steps=[\n                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n                ('encoder', encoders[key]())\n            ]\n        )    \n\n        numeric_transformer = Pipeline(\n            steps=[\n                ('imputer', SimpleImputer(strategy='mean')),\n                ('scaler', StandardScaler())\n            ]\n        )\n\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer, num_columns),\n                ('categorical', categorical_transformer, cat_columns)\n            ]\n        )\n\n        pipe_XGBR = Pipeline(\n            steps=[\n                ('preprocessor', preprocessor),\n                ('classifier',XGBR)\n            ]\n        )\n\n        modelpipe_XGBR = pipe_XGBR.fit(X_train, y_train)\n        y_pred = modelpipe_XGBR.predict(X_test)\n\n        row = {\n            'encoder': key,\n            'r2': modelpipe_XGBR.score(X_test, y_test),\n            'mse': mean_squared_error(y_test, y_pred, squared=True),\n            'rmse': mean_squared_error(y_test, y_pred,squared=False)\n        }\n        df_resultsXGBR = df_resultsXGBR.append(row, ignore_index=True)\n    except :\n        raw={\n            'encoder': key,\n            'r2': np.nan,\n            'mse': np.nan,\n            'rmse': np.nan\n        }\n        df_resultsXGBR = df_resultsXGBR.append(row, ignore_index=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T17:54:11.307943Z","iopub.execute_input":"2021-08-19T17:54:11.308306Z","iopub.status.idle":"2021-08-19T18:02:00.545854Z","shell.execute_reply.started":"2021-08-19T17:54:11.308268Z","shell.execute_reply":"2021-08-19T18:02:00.544829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_resultsXGBR.head(20).sort_values(by='rmse')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T18:02:00.547738Z","iopub.execute_input":"2021-08-19T18:02:00.548115Z","iopub.status.idle":"2021-08-19T18:02:00.565965Z","shell.execute_reply.started":"2021-08-19T18:02:00.548065Z","shell.execute_reply":"2021-08-19T18:02:00.565126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBR :\n\n    encoder \tr2 \tmse \trmse\n    12 \tSumEncoder \t0.067784 \t0.523500 \t0.723533\n    6 \tJamesSteinEncoder \t0.067728 \t0.523532 \t0.723555\n    9 \tMEstimateEncoder \t0.067617 \t0.523594 \t0.723598\n    13 \tTargetEncoder \t0.067617 \t0.523594 \t0.723598\n    14 \tTargetEncoder \t0.067617 \t0.523594 \t0.723598\n    7 \tOneHotEncoder \t0.067450 \t0.523688 \t0.723663\n    5 \tHelmertEncoder \t0.067349 \t0.523744 \t0.723702\n    11 \tPolynomialEncoder \t0.067289 \t0.523778 \t0.723725\n    10 \tOrdinalEncoder \t0.067080 \t0.523895 \t0.723806\n    0 \tBackwardDifferenceEncoder \t0.066829 \t0.524036 \t0.723904\n    3 \tCatBoostEncoder \t0.066808 \t0.524048 \t0.723912\n    1 \tBaseNEncoder \t0.066565 \t0.524185 \t0.724006\n    2 \tBinaryEncoder \t0.066565 \t0.524185 \t0.724006\n    4 \tHashingEncoder \t0.063058 \t0.526154 \t0.725365\n    8 \tLeaveOneOutEncoder \t0.000761 \t0.561138 \t0.74909\n","metadata":{}},{"cell_type":"code","source":"Catmodel = CatBoostRegressor(iterations=6800,\n                          learning_rate=0.93,\n                          loss_function=\"RMSE\",\n                          random_state=42,\n                          verbose=0,\n                          thread_count=4,\n                          depth=1,\n                          task_type=\"GPU\",\n                          l2_leaf_reg=3.28,\n                         )\nencoders = {\n    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n    'BaseNEncoder': ce.basen.BaseNEncoder,\n    'BinaryEncoder': ce.binary.BinaryEncoder,\n    'CatBoostEncoder': ce.cat_boost.CatBoostEncoder,\n    'HashingEncoder': ce.hashing.HashingEncoder,\n    'HelmertEncoder': ce.helmert.HelmertEncoder,\n    'JamesSteinEncoder': ce.james_stein.JamesSteinEncoder,\n    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n    'LeaveOneOutEncoder': ce.leave_one_out.LeaveOneOutEncoder,\n    'MEstimateEncoder': ce.m_estimate.MEstimateEncoder,\n    'OrdinalEncoder': ce.ordinal.OrdinalEncoder,\n    'PolynomialEncoder': ce.polynomial.PolynomialEncoder,\n    'SumEncoder': ce.sum_coding.SumEncoder,\n    'TargetEncoder': ce.target_encoder.TargetEncoder,\n    'WOEEncoder': ce.woe.WOEEncoder\n}\ndf_resultsCat = pd.DataFrame(columns=['encoder', 'r2', 'mse', 'rmse'])\n\nfor key in encoders:\n    try:\n        categorical_transformer = Pipeline(\n            steps=[\n                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n                ('encoder', encoders[key]())\n            ]\n        )    \n\n        numeric_transformer = Pipeline(\n            steps=[\n                ('imputer', SimpleImputer(strategy='mean')),\n                ('scaler', StandardScaler())\n            ]\n        )\n\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer, num_columns),\n                ('categorical', categorical_transformer, cat_columns)\n            ]\n        )\n\n        pipe_Cat = Pipeline(\n            steps=[\n                ('preprocessor', preprocessor),\n                ('classifier', Catmodel)\n            ]\n        )\n\n        modelpipe_Cat = pipe_Cat.fit(X_train, y_train)\n        y_pred = modelpipe_Cat.predict(X_test)\n\n        row = {'encoder': key,\n            'r2': modelpipe_Cat.score(X_test, y_test),\n            'mse': mean_squared_error(y_test, y_pred, squared=True),\n            'rmse': mean_squared_error(y_test, y_pred,squared=False)\n        }\n        df_resultsCat = df_resultsCat.append(row, ignore_index=True)\n    except:\n        raw={'encoder': key,\n        'r2': np.nan,\n        'mse': np.nan,\n        'rmse': np.nan}\n        df_resultsCat = df_resultsCat.append(row, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T18:08:16.318045Z","iopub.execute_input":"2021-08-19T18:08:16.318409Z","iopub.status.idle":"2021-08-19T18:17:53.216553Z","shell.execute_reply.started":"2021-08-19T18:08:16.318378Z","shell.execute_reply":"2021-08-19T18:17:53.215454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_resultsCat.head(20).sort_values(by='rmse')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T18:17:53.218404Z","iopub.execute_input":"2021-08-19T18:17:53.218752Z","iopub.status.idle":"2021-08-19T18:17:53.233099Z","shell.execute_reply.started":"2021-08-19T18:17:53.218708Z","shell.execute_reply":"2021-08-19T18:17:53.23228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    OrderedDict([('colsample_bytree', 0.4481260135905676),\n                 ('learning_rate', 0.01),\n                 ('max_bin', 594),\n                 ('max_depth', 11),\n                 ('min_child_samples', 83),\n                 ('min_child_weight', 8.65733341724971),\n                 ('n_estimators', 2693),\n                 ('num_leaves', 512),\n                 ('objective', 'tweedie'),\n                 ('reg_alpha', 0.0049073005447637175),\n                 ('reg_lambda', 1e-09),\n                 ('reg_sqrt', True),\n                 ('subsample', 0.4611042237962284),\n                  ('subsample_freq', 2)])\n      params = {\n     'max_bin': 500,\n    'feature_fraction': 0.78,\n    'bagging_fraction': 0.78,\n    'objective': 'regression',\n    'max_depth': -1,\n    'learning_rate': 0.01,\n    \"bagging_seed\": 42,\n    'random_state': 42,\n    \"metric\": 'mse',\n    \"verbosity\": -1,\n}       \n\n\n        params_lgb = {\n            'application': 'regression',\n            'metric': 'rmse',\n            'learning_rate': 0.01,\n            'boosting': 'gbdt',\n            'verbose': -1,\n            'num_leaves': 293, \n            'feature_fraction': 0.7649753076229889, \n            'bagging_fraction': 0.7888028266800043, \n            'lambda_l1': 7.882036517237124e-05, \n            'lambda_l2': 1.2301397753601606e-08, \n            'min_split_gain': 0.3615899594462653\n        }\n\n\n             \n                params = {\n    \"objective\": \"rmse\",\n    \"metric\": \"rmse\",\n    \"boosting_type\": 'gbdt', #\"gbdt\", \"dart\"\n    #\"xgboost_dart_mode\": True,\n    'device_type': 'cpu', #'gpu', 'cuda'\n    #'drop_rate': 0.5, #For dart type only\n    'seed': seed,\n    #'deterministic': True,\n    'tree_learner': 'data',\n    'num_threads': 4,\n    'max_bin': 512,\n    'max_depth': 20,\n    'num_leaves': 250,\n    'early_stopping_rounds': 500,\n    'lambda_l1': 1.75,\n    'lambda_l2': 3.25,\n    'cat_l2': 1.25,\n    'learning_rate': 0.005,\n    'feature_fraction': 0.25,\n    \"feature_fraction_seed\": seed,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 50,\n    'bagging_seed': seed,\n    'force_col_wise': True,\n    #'min_data_in_leaf': 27,\n    'path_smooth': 2.5,\n    'cat_smooth': 2.5,\n  }\n                \n       lgbm_params = {\n        \"random_state\": 2021,\n        \"metric\": \"rmse\",\n        \"n_jobs\": -1,\n        \"early_stopping_round\": 350,\n        \"reg_lambda\": 0.01,\n        \"colsample_bytree\": 0.33,\n        \"learning_rate\": 0.005,\n        \"max_depth\": 65,\n        \"num_leaves\": 250,\n        \"min_child_samples\": 100,\n        \"n_estimators\": 2000,\n        \"cat_smooth\": 50,\n        \"max_bin\": 537,\n        \"min_data_per_group\": 117,\n        \"bagging_freq\": 25,\n        \"bagging_fraction\": 0.6709049555262285,\n        \"cat_l2\": 7.5586732660804445,\n    }\n              \n# Transferring the best parameters to our basic regressor\nreg = lgb.LGBMRegressor(boosting_type='gbdt',\n                        n_jobs=-1, \n                        verbose=-1,\n                        random_state=0,\n                         **best_params)","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgbm\nseed = np.random.randint(1, 1e5)\nlgbm_params = {\n        \"random_state\": 2021,\n        \"metric\": \"rmse\",\n        \"reg_lambda\": 0.01,\n        \"colsample_bytree\": 0.33,\n        \"learning_rate\": 0.005,\n        \"max_depth\": 65,\n        \"num_leaves\": 250,\n        \"min_child_samples\": 100,\n        \"n_estimators\": 2000,\n        \"cat_smooth\": 50,\n        \"min_data_per_group\": 117,\n        \"cat_l2\": 7.5586732660804445,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0}\n              \n# Transferring the best parameters to our basic regressor\nlgbmreg = lgbm.LGBMRegressor(boosting_type='gbdt',\n                        n_jobs=-1, \n                        verbose=-1,\n                         **lgbm_params)\nencoders = {\n    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n    'BaseNEncoder': ce.basen.BaseNEncoder,\n    'BinaryEncoder': ce.binary.BinaryEncoder,\n    'CatBoostEncoder': ce.cat_boost.CatBoostEncoder,\n    'HashingEncoder': ce.hashing.HashingEncoder,\n    'HelmertEncoder': ce.helmert.HelmertEncoder,\n    'JamesSteinEncoder': ce.james_stein.JamesSteinEncoder,\n    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n    'LeaveOneOutEncoder': ce.leave_one_out.LeaveOneOutEncoder,\n    'MEstimateEncoder': ce.m_estimate.MEstimateEncoder,\n    'OrdinalEncoder': ce.ordinal.OrdinalEncoder,\n    'PolynomialEncoder': ce.polynomial.PolynomialEncoder,\n    'SumEncoder': ce.sum_coding.SumEncoder,\n    'TargetEncoder': ce.target_encoder.TargetEncoder,\n    'WOEEncoder': ce.woe.WOEEncoder\n}\ndf_resultslgbmreg = pd.DataFrame(columns=['encoder', 'r2', 'mse', 'rmse'])\n\nfor key in encoders:\n    try:\n        categorical_transformer = Pipeline(\n        steps=[\n            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n            ('encoder', encoders[key]())\n        ]\n        )    \n\n        numeric_transformer = Pipeline(\n        steps=[\n            ('imputer', SimpleImputer(strategy='mean')),\n            ('scaler', StandardScaler())\n        ]\n        )\n\n        preprocessor = ColumnTransformer(\n        transformers=[\n            ('numerical', numeric_transformer, num_columns),\n            ('categorical', categorical_transformer, cat_columns)\n        ]\n        )\n\n        pipe_lgbmreg = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor),\n            ('classifier', lgbmreg)\n        ]\n        )\n\n        modelpipe_lgbmreg = pipe_lgbmreg.fit(X_train, y_train)\n        y_pred = modelpipe_lgbmreg.predict(X_test)\n\n        row = {'encoder': key,\n        'r2': modelpipe_lgbmreg.score(X_test, y_test),\n        'mse': mean_squared_error(y_test, y_pred, squared=True),\n        'rmse': mean_squared_error(y_test, y_pred,squared=False)\n        }\n        df_resultslgbmreg = df_resultslgbmreg.append(row, ignore_index=True)\n    except:\n        raw={'encoder': key,\n        'r2': np.nan,\n        'mse': np.nan,\n        'rmse': np.nan}\n        df_resultslgbmreg = df_resultslgbmreg.append(row, ignore_index=True)   ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T19:19:06.347369Z","iopub.execute_input":"2021-08-19T19:19:06.347756Z","iopub.status.idle":"2021-08-19T19:50:09.939133Z","shell.execute_reply.started":"2021-08-19T19:19:06.347722Z","shell.execute_reply":"2021-08-19T19:50:09.938155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_resultslgbmreg.head(20).sort_values(by='rmse')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T19:50:09.94125Z","iopub.execute_input":"2021-08-19T19:50:09.941614Z","iopub.status.idle":"2021-08-19T19:50:09.9558Z","shell.execute_reply.started":"2021-08-19T19:50:09.941568Z","shell.execute_reply":"2021-08-19T19:50:09.954983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"        LGBM:\n        \n       encoder \tr2 \tmse \trmse\n        12 \tSumEncoder \t0.068020 \t0.523367 \t0.723441\n        2 \tBinaryEncoder \t0.067894 \t0.523438 \t0.723490\n        1 \tBaseNEncoder \t0.067690 \t0.523553 \t0.723570\n        7 \tOneHotEncoder \t0.067670 \t0.523564 \t0.723577\n        5 \tHelmertEncoder \t0.067628 \t0.523588 \t0.723594\n        0 \tBackwardDifferenceEncoder \t0.067431 \t0.523698 \t0.723670\n        11 \tPolynomialEncoder \t0.066782 \t0.524063 \t0.723922\n        13 \tTargetEncoder \t0.065995 \t0.524505 \t0.724227\n        14 \tTargetEncoder \t0.065995 \t0.524505 \t0.724227\n        9 \tMEstimateEncoder \t0.065979 \t0.524514 \t0.724233\n        10 \tOrdinalEncoder \t0.065936 \t0.524538 \t0.724250\n        6 \tJamesSteinEncoder \t0.065931 \t0.524541 \t0.724252\n        3 \tCatBoostEncoder \t0.064839 \t0.525154 \t0.724675\n        4 \tHashingEncoder \t0.063647 \t0.525823 \t0.725137\n        8 \tLeaveOneOutEncoder \t0.000478 \t0.561297 \t0.749197","metadata":{}},{"cell_type":"markdown","source":"# COnclusion : \nfeatures to reduce : \ncont7 can be removed  : low variance \nlist to drop ['cat0', 'cat2', 'cont1', 'cat4'] less voted by algho .\ni shoyld take a view in this notebook :\n\nhttps://www.kaggle.com/dwin183287/30-days-of-ml-eda\nhttps://www.kaggle.com/vipin20/getting-start-30-days-ml\n","metadata":{}}]}